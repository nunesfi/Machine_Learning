{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from yellowbrick.classifier import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASE CREDIT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nunesfi/Desktop/IA/files/Bases de dados-20230823T013645Z-001/Bases de dados/credit.pkl', 'rb') as f:\n",
    "    x_credit_treinamento, y_credit_treinamento, x_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.52588457\n",
      "Iteration 2, loss = 0.52409529\n",
      "Iteration 3, loss = 0.52234362\n",
      "Iteration 4, loss = 0.52066659\n",
      "Iteration 5, loss = 0.51901416\n",
      "Iteration 6, loss = 0.51740771\n",
      "Iteration 7, loss = 0.51580203\n",
      "Iteration 8, loss = 0.51426281\n",
      "Iteration 9, loss = 0.51276238\n",
      "Iteration 10, loss = 0.51125135\n",
      "Iteration 11, loss = 0.50974811\n",
      "Iteration 12, loss = 0.50829686\n",
      "Iteration 13, loss = 0.50680735\n",
      "Iteration 14, loss = 0.50537133\n",
      "Iteration 15, loss = 0.50397767\n",
      "Iteration 16, loss = 0.50257871\n",
      "Iteration 17, loss = 0.50118801\n",
      "Iteration 18, loss = 0.49983832\n",
      "Iteration 19, loss = 0.49845336\n",
      "Iteration 20, loss = 0.49713405\n",
      "Iteration 21, loss = 0.49579969\n",
      "Iteration 22, loss = 0.49451672\n",
      "Iteration 23, loss = 0.49322735\n",
      "Iteration 24, loss = 0.49199967\n",
      "Iteration 25, loss = 0.49073042\n",
      "Iteration 26, loss = 0.48951474\n",
      "Iteration 27, loss = 0.48830030\n",
      "Iteration 28, loss = 0.48713676\n",
      "Iteration 29, loss = 0.48595730\n",
      "Iteration 30, loss = 0.48483624\n",
      "Iteration 31, loss = 0.48369153\n",
      "Iteration 32, loss = 0.48256780\n",
      "Iteration 33, loss = 0.48144448\n",
      "Iteration 34, loss = 0.48037574\n",
      "Iteration 35, loss = 0.47931492\n",
      "Iteration 36, loss = 0.47819784\n",
      "Iteration 37, loss = 0.47722991\n",
      "Iteration 38, loss = 0.47621304\n",
      "Iteration 39, loss = 0.47516959\n",
      "Iteration 40, loss = 0.47416701\n",
      "Iteration 41, loss = 0.47318754\n",
      "Iteration 42, loss = 0.47219954\n",
      "Iteration 43, loss = 0.47125265\n",
      "Iteration 44, loss = 0.47030228\n",
      "Iteration 45, loss = 0.46934536\n",
      "Iteration 46, loss = 0.46843025\n",
      "Iteration 47, loss = 0.46751531\n",
      "Iteration 48, loss = 0.46661948\n",
      "Iteration 49, loss = 0.46576503\n",
      "Iteration 50, loss = 0.46491698\n",
      "Iteration 51, loss = 0.46404786\n",
      "Iteration 52, loss = 0.46321135\n",
      "Iteration 53, loss = 0.46237250\n",
      "Iteration 54, loss = 0.46153122\n",
      "Iteration 55, loss = 0.46076918\n",
      "Iteration 56, loss = 0.45997004\n",
      "Iteration 57, loss = 0.45919692\n",
      "Iteration 58, loss = 0.45841952\n",
      "Iteration 59, loss = 0.45766326\n",
      "Iteration 60, loss = 0.45690821\n",
      "Iteration 61, loss = 0.45616809\n",
      "Iteration 62, loss = 0.45548884\n",
      "Iteration 63, loss = 0.45473876\n",
      "Iteration 64, loss = 0.45404596\n",
      "Iteration 65, loss = 0.45337264\n",
      "Iteration 66, loss = 0.45270507\n",
      "Iteration 67, loss = 0.45202915\n",
      "Iteration 68, loss = 0.45139814\n",
      "Iteration 69, loss = 0.45073469\n",
      "Iteration 70, loss = 0.45009390\n",
      "Iteration 71, loss = 0.44945598\n",
      "Iteration 72, loss = 0.44881470\n",
      "Iteration 73, loss = 0.44821873\n",
      "Iteration 74, loss = 0.44761480\n",
      "Iteration 75, loss = 0.44700484\n",
      "Iteration 76, loss = 0.44641032\n",
      "Iteration 77, loss = 0.44583743\n",
      "Iteration 78, loss = 0.44527203\n",
      "Iteration 79, loss = 0.44471429\n",
      "Iteration 80, loss = 0.44414712\n",
      "Iteration 81, loss = 0.44360885\n",
      "Iteration 82, loss = 0.44307955\n",
      "Iteration 83, loss = 0.44255774\n",
      "Iteration 84, loss = 0.44204308\n",
      "Iteration 85, loss = 0.44149373\n",
      "Iteration 86, loss = 0.44100488\n",
      "Iteration 87, loss = 0.44050566\n",
      "Iteration 88, loss = 0.44000119\n",
      "Iteration 89, loss = 0.43949793\n",
      "Iteration 90, loss = 0.43904223\n",
      "Iteration 91, loss = 0.43854604\n",
      "Iteration 92, loss = 0.43807064\n",
      "Iteration 93, loss = 0.43758261\n",
      "Iteration 94, loss = 0.43715715\n",
      "Iteration 95, loss = 0.43665293\n",
      "Iteration 96, loss = 0.43621501\n",
      "Iteration 97, loss = 0.43572713\n",
      "Iteration 98, loss = 0.43527338\n",
      "Iteration 99, loss = 0.43483670\n",
      "Iteration 100, loss = 0.43438667\n",
      "Iteration 101, loss = 0.43395506\n",
      "Iteration 102, loss = 0.43353992\n",
      "Iteration 103, loss = 0.43309700\n",
      "Iteration 104, loss = 0.43271582\n",
      "Iteration 105, loss = 0.43230722\n",
      "Iteration 106, loss = 0.43191171\n",
      "Iteration 107, loss = 0.43154762\n",
      "Iteration 108, loss = 0.43113917\n",
      "Iteration 109, loss = 0.43078367\n",
      "Iteration 110, loss = 0.43040984\n",
      "Iteration 111, loss = 0.43000921\n",
      "Iteration 112, loss = 0.42961130\n",
      "Iteration 113, loss = 0.42925791\n",
      "Iteration 114, loss = 0.42884650\n",
      "Iteration 115, loss = 0.42845560\n",
      "Iteration 116, loss = 0.42804231\n",
      "Iteration 117, loss = 0.42767546\n",
      "Iteration 118, loss = 0.42724568\n",
      "Iteration 119, loss = 0.42687847\n",
      "Iteration 120, loss = 0.42648272\n",
      "Iteration 121, loss = 0.42609370\n",
      "Iteration 122, loss = 0.42570522\n",
      "Iteration 123, loss = 0.42530681\n",
      "Iteration 124, loss = 0.42491791\n",
      "Iteration 125, loss = 0.42451846\n",
      "Iteration 126, loss = 0.42409420\n",
      "Iteration 127, loss = 0.42368387\n",
      "Iteration 128, loss = 0.42325285\n",
      "Iteration 129, loss = 0.42287935\n",
      "Iteration 130, loss = 0.42238473\n",
      "Iteration 131, loss = 0.42198367\n",
      "Iteration 132, loss = 0.42152288\n",
      "Iteration 133, loss = 0.42109403\n",
      "Iteration 134, loss = 0.42065142\n",
      "Iteration 135, loss = 0.42026026\n",
      "Iteration 136, loss = 0.41971319\n",
      "Iteration 137, loss = 0.41920560\n",
      "Iteration 138, loss = 0.41869523\n",
      "Iteration 139, loss = 0.41815257\n",
      "Iteration 140, loss = 0.41762332\n",
      "Iteration 141, loss = 0.41704991\n",
      "Iteration 142, loss = 0.41642122\n",
      "Iteration 143, loss = 0.41572801\n",
      "Iteration 144, loss = 0.41489537\n",
      "Iteration 145, loss = 0.41394940\n",
      "Iteration 146, loss = 0.41289597\n",
      "Iteration 147, loss = 0.41175168\n",
      "Iteration 148, loss = 0.41051621\n",
      "Iteration 149, loss = 0.40928950\n",
      "Iteration 150, loss = 0.40794485\n",
      "Iteration 151, loss = 0.40640348\n",
      "Iteration 152, loss = 0.40480607\n",
      "Iteration 153, loss = 0.40305954\n",
      "Iteration 154, loss = 0.40133411\n",
      "Iteration 155, loss = 0.39948312\n",
      "Iteration 156, loss = 0.39769380\n",
      "Iteration 157, loss = 0.39577888\n",
      "Iteration 158, loss = 0.39359740\n",
      "Iteration 159, loss = 0.39112188\n",
      "Iteration 160, loss = 0.38879298\n",
      "Iteration 161, loss = 0.38624826\n",
      "Iteration 162, loss = 0.38392038\n",
      "Iteration 163, loss = 0.38144162\n",
      "Iteration 164, loss = 0.37902019\n",
      "Iteration 165, loss = 0.37657640\n",
      "Iteration 166, loss = 0.37424288\n",
      "Iteration 167, loss = 0.37199418\n",
      "Iteration 168, loss = 0.36981875\n",
      "Iteration 169, loss = 0.36764724\n",
      "Iteration 170, loss = 0.36546729\n",
      "Iteration 171, loss = 0.36346718\n",
      "Iteration 172, loss = 0.36155568\n",
      "Iteration 173, loss = 0.35974904\n",
      "Iteration 174, loss = 0.35802311\n",
      "Iteration 175, loss = 0.35627603\n",
      "Iteration 176, loss = 0.35462378\n",
      "Iteration 177, loss = 0.35291291\n",
      "Iteration 178, loss = 0.35117143\n",
      "Iteration 179, loss = 0.34953320\n",
      "Iteration 180, loss = 0.34788112\n",
      "Iteration 181, loss = 0.34616892\n",
      "Iteration 182, loss = 0.34450296\n",
      "Iteration 183, loss = 0.34278573\n",
      "Iteration 184, loss = 0.34115274\n",
      "Iteration 185, loss = 0.33957468\n",
      "Iteration 186, loss = 0.33805422\n",
      "Iteration 187, loss = 0.33646952\n",
      "Iteration 188, loss = 0.33485518\n",
      "Iteration 189, loss = 0.33316391\n",
      "Iteration 190, loss = 0.33152883\n",
      "Iteration 191, loss = 0.32993565\n",
      "Iteration 192, loss = 0.32837301\n",
      "Iteration 193, loss = 0.32674903\n",
      "Iteration 194, loss = 0.32508913\n",
      "Iteration 195, loss = 0.32353925\n",
      "Iteration 196, loss = 0.32193622\n",
      "Iteration 197, loss = 0.32022273\n",
      "Iteration 198, loss = 0.31847235\n",
      "Iteration 199, loss = 0.31672820\n",
      "Iteration 200, loss = 0.31495785\n",
      "Iteration 201, loss = 0.31314744\n",
      "Iteration 202, loss = 0.31135252\n",
      "Iteration 203, loss = 0.30944718\n",
      "Iteration 204, loss = 0.30739819\n",
      "Iteration 205, loss = 0.30536285\n",
      "Iteration 206, loss = 0.30336027\n",
      "Iteration 207, loss = 0.30129982\n",
      "Iteration 208, loss = 0.29914588\n",
      "Iteration 209, loss = 0.29696903\n",
      "Iteration 210, loss = 0.29475143\n",
      "Iteration 211, loss = 0.29256776\n",
      "Iteration 212, loss = 0.29043556\n",
      "Iteration 213, loss = 0.28820242\n",
      "Iteration 214, loss = 0.28593229\n",
      "Iteration 215, loss = 0.28343982\n",
      "Iteration 216, loss = 0.28095049\n",
      "Iteration 217, loss = 0.27825476\n",
      "Iteration 218, loss = 0.27568458\n",
      "Iteration 219, loss = 0.27314855\n",
      "Iteration 220, loss = 0.27059301\n",
      "Iteration 221, loss = 0.26790655\n",
      "Iteration 222, loss = 0.26520356\n",
      "Iteration 223, loss = 0.26253739\n",
      "Iteration 224, loss = 0.25985703\n",
      "Iteration 225, loss = 0.25767245\n",
      "Iteration 226, loss = 0.25559193\n",
      "Iteration 227, loss = 0.25365423\n",
      "Iteration 228, loss = 0.25181148\n",
      "Iteration 229, loss = 0.25009619\n",
      "Iteration 230, loss = 0.24848921\n",
      "Iteration 231, loss = 0.24682720\n",
      "Iteration 232, loss = 0.24524392\n",
      "Iteration 233, loss = 0.24389155\n",
      "Iteration 234, loss = 0.24252236\n",
      "Iteration 235, loss = 0.24131802\n",
      "Iteration 236, loss = 0.24011413\n",
      "Iteration 237, loss = 0.23901589\n",
      "Iteration 238, loss = 0.23793027\n",
      "Iteration 239, loss = 0.23680442\n",
      "Iteration 240, loss = 0.23569346\n",
      "Iteration 241, loss = 0.23461849\n",
      "Iteration 242, loss = 0.23363550\n",
      "Iteration 243, loss = 0.23260040\n",
      "Iteration 244, loss = 0.23164130\n",
      "Iteration 245, loss = 0.23067099\n",
      "Iteration 246, loss = 0.22973874\n",
      "Iteration 247, loss = 0.22877394\n",
      "Iteration 248, loss = 0.22786398\n",
      "Iteration 249, loss = 0.22697421\n",
      "Iteration 250, loss = 0.22610422\n",
      "Iteration 251, loss = 0.22521820\n",
      "Iteration 252, loss = 0.22438849\n",
      "Iteration 253, loss = 0.22349631\n",
      "Iteration 254, loss = 0.22271708\n",
      "Iteration 255, loss = 0.22187036\n",
      "Iteration 256, loss = 0.22108411\n",
      "Iteration 257, loss = 0.22030481\n",
      "Iteration 258, loss = 0.21955060\n",
      "Iteration 259, loss = 0.21877271\n",
      "Iteration 260, loss = 0.21796949\n",
      "Iteration 261, loss = 0.21723310\n",
      "Iteration 262, loss = 0.21650346\n",
      "Iteration 263, loss = 0.21574757\n",
      "Iteration 264, loss = 0.21502352\n",
      "Iteration 265, loss = 0.21429950\n",
      "Iteration 266, loss = 0.21358729\n",
      "Iteration 267, loss = 0.21285609\n",
      "Iteration 268, loss = 0.21216577\n",
      "Iteration 269, loss = 0.21145137\n",
      "Iteration 270, loss = 0.21081022\n",
      "Iteration 271, loss = 0.21008791\n",
      "Iteration 272, loss = 0.20942639\n",
      "Iteration 273, loss = 0.20875247\n",
      "Iteration 274, loss = 0.20807325\n",
      "Iteration 275, loss = 0.20739931\n",
      "Iteration 276, loss = 0.20674798\n",
      "Iteration 277, loss = 0.20609697\n",
      "Iteration 278, loss = 0.20544275\n",
      "Iteration 279, loss = 0.20481657\n",
      "Iteration 280, loss = 0.20417504\n",
      "Iteration 281, loss = 0.20357567\n",
      "Iteration 282, loss = 0.20292698\n",
      "Iteration 283, loss = 0.20228554\n",
      "Iteration 284, loss = 0.20168279\n",
      "Iteration 285, loss = 0.20106592\n",
      "Iteration 286, loss = 0.20041715\n",
      "Iteration 287, loss = 0.19984104\n",
      "Iteration 288, loss = 0.19921415\n",
      "Iteration 289, loss = 0.19862706\n",
      "Iteration 290, loss = 0.19801496\n",
      "Iteration 291, loss = 0.19743368\n",
      "Iteration 292, loss = 0.19686130\n",
      "Iteration 293, loss = 0.19626368\n",
      "Iteration 294, loss = 0.19567078\n",
      "Iteration 295, loss = 0.19509626\n",
      "Iteration 296, loss = 0.19451959\n",
      "Iteration 297, loss = 0.19392783\n",
      "Iteration 298, loss = 0.19335278\n",
      "Iteration 299, loss = 0.19283080\n",
      "Iteration 300, loss = 0.19223861\n",
      "Iteration 301, loss = 0.19169183\n",
      "Iteration 302, loss = 0.19111841\n",
      "Iteration 303, loss = 0.19056221\n",
      "Iteration 304, loss = 0.19000615\n",
      "Iteration 305, loss = 0.18948482\n",
      "Iteration 306, loss = 0.18891811\n",
      "Iteration 307, loss = 0.18836175\n",
      "Iteration 308, loss = 0.18785660\n",
      "Iteration 309, loss = 0.18733495\n",
      "Iteration 310, loss = 0.18680992\n",
      "Iteration 311, loss = 0.18627617\n",
      "Iteration 312, loss = 0.18577377\n",
      "Iteration 313, loss = 0.18526120\n",
      "Iteration 314, loss = 0.18474141\n",
      "Iteration 315, loss = 0.18426220\n",
      "Iteration 316, loss = 0.18373878\n",
      "Iteration 317, loss = 0.18325585\n",
      "Iteration 318, loss = 0.18273348\n",
      "Iteration 319, loss = 0.18226764\n",
      "Iteration 320, loss = 0.18178235\n",
      "Iteration 321, loss = 0.18127782\n",
      "Iteration 322, loss = 0.18085050\n",
      "Iteration 323, loss = 0.18032440\n",
      "Iteration 324, loss = 0.17985877\n",
      "Iteration 325, loss = 0.17936411\n",
      "Iteration 326, loss = 0.17889377\n",
      "Iteration 327, loss = 0.17842503\n",
      "Iteration 328, loss = 0.17793989\n",
      "Iteration 329, loss = 0.17750355\n",
      "Iteration 330, loss = 0.17699882\n",
      "Iteration 331, loss = 0.17656141\n",
      "Iteration 332, loss = 0.17609154\n",
      "Iteration 333, loss = 0.17563814\n",
      "Iteration 334, loss = 0.17520688\n",
      "Iteration 335, loss = 0.17475188\n",
      "Iteration 336, loss = 0.17429520\n",
      "Iteration 337, loss = 0.17383291\n",
      "Iteration 338, loss = 0.17338784\n",
      "Iteration 339, loss = 0.17299271\n",
      "Iteration 340, loss = 0.17253994\n",
      "Iteration 341, loss = 0.17212154\n",
      "Iteration 342, loss = 0.17169792\n",
      "Iteration 343, loss = 0.17128624\n",
      "Iteration 344, loss = 0.17087022\n",
      "Iteration 345, loss = 0.17046112\n",
      "Iteration 346, loss = 0.17003324\n",
      "Iteration 347, loss = 0.16964985\n",
      "Iteration 348, loss = 0.16922221\n",
      "Iteration 349, loss = 0.16883996\n",
      "Iteration 350, loss = 0.16841989\n",
      "Iteration 351, loss = 0.16800068\n",
      "Iteration 352, loss = 0.16761017\n",
      "Iteration 353, loss = 0.16729289\n",
      "Iteration 354, loss = 0.16680610\n",
      "Iteration 355, loss = 0.16649604\n",
      "Iteration 356, loss = 0.16601578\n",
      "Iteration 357, loss = 0.16565321\n",
      "Iteration 358, loss = 0.16526390\n",
      "Iteration 359, loss = 0.16487985\n",
      "Iteration 360, loss = 0.16448883\n",
      "Iteration 361, loss = 0.16411740\n",
      "Iteration 362, loss = 0.16375363\n",
      "Iteration 363, loss = 0.16338599\n",
      "Iteration 364, loss = 0.16300243\n",
      "Iteration 365, loss = 0.16261976\n",
      "Iteration 366, loss = 0.16226292\n",
      "Iteration 367, loss = 0.16187007\n",
      "Iteration 368, loss = 0.16151175\n",
      "Iteration 369, loss = 0.16114935\n",
      "Iteration 370, loss = 0.16078470\n",
      "Iteration 371, loss = 0.16044637\n",
      "Iteration 372, loss = 0.16010326\n",
      "Iteration 373, loss = 0.15973330\n",
      "Iteration 374, loss = 0.15941891\n",
      "Iteration 375, loss = 0.15907053\n",
      "Iteration 376, loss = 0.15865964\n",
      "Iteration 377, loss = 0.15833242\n",
      "Iteration 378, loss = 0.15798829\n",
      "Iteration 379, loss = 0.15761236\n",
      "Iteration 380, loss = 0.15720262\n",
      "Iteration 381, loss = 0.15688936\n",
      "Iteration 382, loss = 0.15651010\n",
      "Iteration 383, loss = 0.15618955\n",
      "Iteration 384, loss = 0.15582685\n",
      "Iteration 385, loss = 0.15552836\n",
      "Iteration 386, loss = 0.15518211\n",
      "Iteration 387, loss = 0.15483405\n",
      "Iteration 388, loss = 0.15448950\n",
      "Iteration 389, loss = 0.15425506\n",
      "Iteration 390, loss = 0.15384926\n",
      "Iteration 391, loss = 0.15350446\n",
      "Iteration 392, loss = 0.15319804\n",
      "Iteration 393, loss = 0.15289762\n",
      "Iteration 394, loss = 0.15252520\n",
      "Iteration 395, loss = 0.15222905\n",
      "Iteration 396, loss = 0.15193697\n",
      "Iteration 397, loss = 0.15163072\n",
      "Iteration 398, loss = 0.15133003\n",
      "Iteration 399, loss = 0.15102847\n",
      "Iteration 400, loss = 0.15074416\n",
      "Iteration 401, loss = 0.15045175\n",
      "Iteration 402, loss = 0.15014582\n",
      "Iteration 403, loss = 0.14982324\n",
      "Iteration 404, loss = 0.14950952\n",
      "Iteration 405, loss = 0.14921925\n",
      "Iteration 406, loss = 0.14895715\n",
      "Iteration 407, loss = 0.14867367\n",
      "Iteration 408, loss = 0.14837682\n",
      "Iteration 409, loss = 0.14809533\n",
      "Iteration 410, loss = 0.14780997\n",
      "Iteration 411, loss = 0.14752470\n",
      "Iteration 412, loss = 0.14724393\n",
      "Iteration 413, loss = 0.14694644\n",
      "Iteration 414, loss = 0.14670200\n",
      "Iteration 415, loss = 0.14642044\n",
      "Iteration 416, loss = 0.14616296\n",
      "Iteration 417, loss = 0.14586849\n",
      "Iteration 418, loss = 0.14571185\n",
      "Iteration 419, loss = 0.14537157\n",
      "Iteration 420, loss = 0.14505061\n",
      "Iteration 421, loss = 0.14479208\n",
      "Iteration 422, loss = 0.14455615\n",
      "Iteration 423, loss = 0.14427991\n",
      "Iteration 424, loss = 0.14399708\n",
      "Iteration 425, loss = 0.14370663\n",
      "Iteration 426, loss = 0.14346289\n",
      "Iteration 427, loss = 0.14316235\n",
      "Iteration 428, loss = 0.14293001\n",
      "Iteration 429, loss = 0.14265008\n",
      "Iteration 430, loss = 0.14238888\n",
      "Iteration 431, loss = 0.14213893\n",
      "Iteration 432, loss = 0.14188792\n",
      "Iteration 433, loss = 0.14161126\n",
      "Iteration 434, loss = 0.14135681\n",
      "Iteration 435, loss = 0.14108096\n",
      "Iteration 436, loss = 0.14089698\n",
      "Iteration 437, loss = 0.14062410\n",
      "Iteration 438, loss = 0.14034477\n",
      "Iteration 439, loss = 0.14008122\n",
      "Iteration 440, loss = 0.13982538\n",
      "Iteration 441, loss = 0.13956407\n",
      "Iteration 442, loss = 0.13933448\n",
      "Iteration 443, loss = 0.13906602\n",
      "Iteration 444, loss = 0.13885986\n",
      "Iteration 445, loss = 0.13857526\n",
      "Iteration 446, loss = 0.13832258\n",
      "Iteration 447, loss = 0.13799820\n",
      "Iteration 448, loss = 0.13771976\n",
      "Iteration 449, loss = 0.13744366\n",
      "Iteration 450, loss = 0.13721989\n",
      "Iteration 451, loss = 0.13689840\n",
      "Iteration 452, loss = 0.13662788\n",
      "Iteration 453, loss = 0.13637239\n",
      "Iteration 454, loss = 0.13606424\n",
      "Iteration 455, loss = 0.13580882\n",
      "Iteration 456, loss = 0.13554495\n",
      "Iteration 457, loss = 0.13528038\n",
      "Iteration 458, loss = 0.13500146\n",
      "Iteration 459, loss = 0.13473765\n",
      "Iteration 460, loss = 0.13446964\n",
      "Iteration 461, loss = 0.13424670\n",
      "Iteration 462, loss = 0.13400184\n",
      "Iteration 463, loss = 0.13373948\n",
      "Iteration 464, loss = 0.13353479\n",
      "Iteration 465, loss = 0.13318576\n",
      "Iteration 466, loss = 0.13292369\n",
      "Iteration 467, loss = 0.13263009\n",
      "Iteration 468, loss = 0.13235017\n",
      "Iteration 469, loss = 0.13209877\n",
      "Iteration 470, loss = 0.13183734\n",
      "Iteration 471, loss = 0.13155584\n",
      "Iteration 472, loss = 0.13135088\n",
      "Iteration 473, loss = 0.13107597\n",
      "Iteration 474, loss = 0.13081016\n",
      "Iteration 475, loss = 0.13055245\n",
      "Iteration 476, loss = 0.13025690\n",
      "Iteration 477, loss = 0.12998119\n",
      "Iteration 478, loss = 0.12972961\n",
      "Iteration 479, loss = 0.12947418\n",
      "Iteration 480, loss = 0.12920625\n",
      "Iteration 481, loss = 0.12899555\n",
      "Iteration 482, loss = 0.12871460\n",
      "Iteration 483, loss = 0.12836396\n",
      "Iteration 484, loss = 0.12807496\n",
      "Iteration 485, loss = 0.12768335\n",
      "Iteration 486, loss = 0.12735073\n",
      "Iteration 487, loss = 0.12704148\n",
      "Iteration 488, loss = 0.12669895\n",
      "Iteration 489, loss = 0.12635264\n",
      "Iteration 490, loss = 0.12598723\n",
      "Iteration 491, loss = 0.12563979\n",
      "Iteration 492, loss = 0.12532941\n",
      "Iteration 493, loss = 0.12496833\n",
      "Iteration 494, loss = 0.12464791\n",
      "Iteration 495, loss = 0.12417887\n",
      "Iteration 496, loss = 0.12382237\n",
      "Iteration 497, loss = 0.12331983\n",
      "Iteration 498, loss = 0.12289969\n",
      "Iteration 499, loss = 0.12248556\n",
      "Iteration 500, loss = 0.12210438\n",
      "Iteration 501, loss = 0.12170434\n",
      "Iteration 502, loss = 0.12129800\n",
      "Iteration 503, loss = 0.12083543\n",
      "Iteration 504, loss = 0.12040065\n",
      "Iteration 505, loss = 0.11995821\n",
      "Iteration 506, loss = 0.11947701\n",
      "Iteration 507, loss = 0.11901260\n",
      "Iteration 508, loss = 0.11850671\n",
      "Iteration 509, loss = 0.11799165\n",
      "Iteration 510, loss = 0.11738482\n",
      "Iteration 511, loss = 0.11669822\n",
      "Iteration 512, loss = 0.11608575\n",
      "Iteration 513, loss = 0.11544917\n",
      "Iteration 514, loss = 0.11470531\n",
      "Iteration 515, loss = 0.11365788\n",
      "Iteration 516, loss = 0.11269106\n",
      "Iteration 517, loss = 0.11176837\n",
      "Iteration 518, loss = 0.11070782\n",
      "Iteration 519, loss = 0.10959903\n",
      "Iteration 520, loss = 0.10840087\n",
      "Iteration 521, loss = 0.10717867\n",
      "Iteration 522, loss = 0.10605566\n",
      "Iteration 523, loss = 0.10498537\n",
      "Iteration 524, loss = 0.10409735\n",
      "Iteration 525, loss = 0.10325537\n",
      "Iteration 526, loss = 0.10264161\n",
      "Iteration 527, loss = 0.10200469\n",
      "Iteration 528, loss = 0.10142365\n",
      "Iteration 529, loss = 0.10089198\n",
      "Iteration 530, loss = 0.10040903\n",
      "Iteration 531, loss = 0.09995797\n",
      "Iteration 532, loss = 0.09956424\n",
      "Iteration 533, loss = 0.09908825\n",
      "Iteration 534, loss = 0.09871523\n",
      "Iteration 535, loss = 0.09825567\n",
      "Iteration 536, loss = 0.09787583\n",
      "Iteration 537, loss = 0.09746645\n",
      "Iteration 538, loss = 0.09710551\n",
      "Iteration 539, loss = 0.09667879\n",
      "Iteration 540, loss = 0.09629586\n",
      "Iteration 541, loss = 0.09590360\n",
      "Iteration 542, loss = 0.09555976\n",
      "Iteration 543, loss = 0.09516684\n",
      "Iteration 544, loss = 0.09484082\n",
      "Iteration 545, loss = 0.09444026\n",
      "Iteration 546, loss = 0.09411298\n",
      "Iteration 547, loss = 0.09369883\n",
      "Iteration 548, loss = 0.09339753\n",
      "Iteration 549, loss = 0.09298537\n",
      "Iteration 550, loss = 0.09260247\n",
      "Iteration 551, loss = 0.09223783\n",
      "Iteration 552, loss = 0.09188899\n",
      "Iteration 553, loss = 0.09153414\n",
      "Iteration 554, loss = 0.09115933\n",
      "Iteration 555, loss = 0.09081245\n",
      "Iteration 556, loss = 0.09047088\n",
      "Iteration 557, loss = 0.09015032\n",
      "Iteration 558, loss = 0.08979050\n",
      "Iteration 559, loss = 0.08944556\n",
      "Iteration 560, loss = 0.08912210\n",
      "Iteration 561, loss = 0.08872945\n",
      "Iteration 562, loss = 0.08843724\n",
      "Iteration 563, loss = 0.08810400\n",
      "Iteration 564, loss = 0.08774503\n",
      "Iteration 565, loss = 0.08742671\n",
      "Iteration 566, loss = 0.08708346\n",
      "Iteration 567, loss = 0.08672297\n",
      "Iteration 568, loss = 0.08642960\n",
      "Iteration 569, loss = 0.08605451\n",
      "Iteration 570, loss = 0.08571516\n",
      "Iteration 571, loss = 0.08541008\n",
      "Iteration 572, loss = 0.08508343\n",
      "Iteration 573, loss = 0.08476115\n",
      "Iteration 574, loss = 0.08444903\n",
      "Iteration 575, loss = 0.08410903\n",
      "Iteration 576, loss = 0.08379351\n",
      "Iteration 577, loss = 0.08348354\n",
      "Iteration 578, loss = 0.08317327\n",
      "Iteration 579, loss = 0.08284939\n",
      "Iteration 580, loss = 0.08252694\n",
      "Iteration 581, loss = 0.08227422\n",
      "Iteration 582, loss = 0.08196852\n",
      "Iteration 583, loss = 0.08163300\n",
      "Iteration 584, loss = 0.08131881\n",
      "Iteration 585, loss = 0.08098240\n",
      "Iteration 586, loss = 0.08066441\n",
      "Iteration 587, loss = 0.08039599\n",
      "Iteration 588, loss = 0.08007886\n",
      "Iteration 589, loss = 0.07977797\n",
      "Iteration 590, loss = 0.07946339\n",
      "Iteration 591, loss = 0.07919825\n",
      "Iteration 592, loss = 0.07888069\n",
      "Iteration 593, loss = 0.07861485\n",
      "Iteration 594, loss = 0.07832756\n",
      "Iteration 595, loss = 0.07801269\n",
      "Iteration 596, loss = 0.07774119\n",
      "Iteration 597, loss = 0.07748629\n",
      "Iteration 598, loss = 0.07714093\n",
      "Iteration 599, loss = 0.07687321\n",
      "Iteration 600, loss = 0.07659836\n",
      "Iteration 601, loss = 0.07627320\n",
      "Iteration 602, loss = 0.07599851\n",
      "Iteration 603, loss = 0.07573074\n",
      "Iteration 604, loss = 0.07548597\n",
      "Iteration 605, loss = 0.07515676\n",
      "Iteration 606, loss = 0.07486019\n",
      "Iteration 607, loss = 0.07458764\n",
      "Iteration 608, loss = 0.07433627\n",
      "Iteration 609, loss = 0.07408022\n",
      "Iteration 610, loss = 0.07379322\n",
      "Iteration 611, loss = 0.07353430\n",
      "Iteration 612, loss = 0.07325829\n",
      "Iteration 613, loss = 0.07301148\n",
      "Iteration 614, loss = 0.07272019\n",
      "Iteration 615, loss = 0.07244985\n",
      "Iteration 616, loss = 0.07221411\n",
      "Iteration 617, loss = 0.07191733\n",
      "Iteration 618, loss = 0.07169876\n",
      "Iteration 619, loss = 0.07142486\n",
      "Iteration 620, loss = 0.07113978\n",
      "Iteration 621, loss = 0.07086701\n",
      "Iteration 622, loss = 0.07060869\n",
      "Iteration 623, loss = 0.07034908\n",
      "Iteration 624, loss = 0.07005874\n",
      "Iteration 625, loss = 0.06975364\n",
      "Iteration 626, loss = 0.06951787\n",
      "Iteration 627, loss = 0.06922590\n",
      "Iteration 628, loss = 0.06890380\n",
      "Iteration 629, loss = 0.06865617\n",
      "Iteration 630, loss = 0.06833285\n",
      "Iteration 631, loss = 0.06807597\n",
      "Iteration 632, loss = 0.06778186\n",
      "Iteration 633, loss = 0.06756893\n",
      "Iteration 634, loss = 0.06728340\n",
      "Iteration 635, loss = 0.06704266\n",
      "Iteration 636, loss = 0.06669602\n",
      "Iteration 637, loss = 0.06646062\n",
      "Iteration 638, loss = 0.06617062\n",
      "Iteration 639, loss = 0.06585767\n",
      "Iteration 640, loss = 0.06564197\n",
      "Iteration 641, loss = 0.06534068\n",
      "Iteration 642, loss = 0.06506826\n",
      "Iteration 643, loss = 0.06482231\n",
      "Iteration 644, loss = 0.06454076\n",
      "Iteration 645, loss = 0.06434043\n",
      "Iteration 646, loss = 0.06409853\n",
      "Iteration 647, loss = 0.06372106\n",
      "Iteration 648, loss = 0.06350515\n",
      "Iteration 649, loss = 0.06331646\n",
      "Iteration 650, loss = 0.06299348\n",
      "Iteration 651, loss = 0.06271483\n",
      "Iteration 652, loss = 0.06245279\n",
      "Iteration 653, loss = 0.06218532\n",
      "Iteration 654, loss = 0.06194619\n",
      "Iteration 655, loss = 0.06163297\n",
      "Iteration 656, loss = 0.06140273\n",
      "Iteration 657, loss = 0.06112820\n",
      "Iteration 658, loss = 0.06088345\n",
      "Iteration 659, loss = 0.06062425\n",
      "Iteration 660, loss = 0.06036500\n",
      "Iteration 661, loss = 0.06020300\n",
      "Iteration 662, loss = 0.05988943\n",
      "Iteration 663, loss = 0.05962479\n",
      "Iteration 664, loss = 0.05940024\n",
      "Iteration 665, loss = 0.05912021\n",
      "Iteration 666, loss = 0.05888002\n",
      "Iteration 667, loss = 0.05865296\n",
      "Iteration 668, loss = 0.05842827\n",
      "Iteration 669, loss = 0.05818740\n",
      "Iteration 670, loss = 0.05797357\n",
      "Iteration 671, loss = 0.05769425\n",
      "Iteration 672, loss = 0.05747939\n",
      "Iteration 673, loss = 0.05721652\n",
      "Iteration 674, loss = 0.05701480\n",
      "Iteration 675, loss = 0.05679242\n",
      "Iteration 676, loss = 0.05663494\n",
      "Iteration 677, loss = 0.05635630\n",
      "Iteration 678, loss = 0.05614167\n",
      "Iteration 679, loss = 0.05596656\n",
      "Iteration 680, loss = 0.05565483\n",
      "Iteration 681, loss = 0.05547756\n",
      "Iteration 682, loss = 0.05525310\n",
      "Iteration 683, loss = 0.05512694\n",
      "Iteration 684, loss = 0.05481548\n",
      "Iteration 685, loss = 0.05459425\n",
      "Iteration 686, loss = 0.05438652\n",
      "Iteration 687, loss = 0.05424310\n",
      "Iteration 688, loss = 0.05395863\n",
      "Iteration 689, loss = 0.05375739\n",
      "Iteration 690, loss = 0.05358328\n",
      "Iteration 691, loss = 0.05336347\n",
      "Iteration 692, loss = 0.05311445\n",
      "Iteration 693, loss = 0.05293726\n",
      "Iteration 694, loss = 0.05271998\n",
      "Iteration 695, loss = 0.05249592\n",
      "Iteration 696, loss = 0.05231505\n",
      "Iteration 697, loss = 0.05210166\n",
      "Iteration 698, loss = 0.05191977\n",
      "Iteration 699, loss = 0.05169596\n",
      "Iteration 700, loss = 0.05150304\n",
      "Iteration 701, loss = 0.05130283\n",
      "Iteration 702, loss = 0.05110833\n",
      "Iteration 703, loss = 0.05087790\n",
      "Iteration 704, loss = 0.05066518\n",
      "Iteration 705, loss = 0.05048729\n",
      "Iteration 706, loss = 0.05032675\n",
      "Iteration 707, loss = 0.05004916\n",
      "Iteration 708, loss = 0.04983881\n",
      "Iteration 709, loss = 0.04960356\n",
      "Iteration 710, loss = 0.04942846\n",
      "Iteration 711, loss = 0.04921642\n",
      "Iteration 712, loss = 0.04903588\n",
      "Iteration 713, loss = 0.04879837\n",
      "Iteration 714, loss = 0.04865126\n",
      "Iteration 715, loss = 0.04846279\n",
      "Iteration 716, loss = 0.04825400\n",
      "Iteration 717, loss = 0.04801691\n",
      "Iteration 718, loss = 0.04779457\n",
      "Iteration 719, loss = 0.04762958\n",
      "Iteration 720, loss = 0.04738304\n",
      "Iteration 721, loss = 0.04723172\n",
      "Iteration 722, loss = 0.04704353\n",
      "Iteration 723, loss = 0.04682130\n",
      "Iteration 724, loss = 0.04665301\n",
      "Iteration 725, loss = 0.04648178\n",
      "Iteration 726, loss = 0.04626274\n",
      "Iteration 727, loss = 0.04608832\n",
      "Iteration 728, loss = 0.04589281\n",
      "Iteration 729, loss = 0.04572217\n",
      "Iteration 730, loss = 0.04547445\n",
      "Iteration 731, loss = 0.04532435\n",
      "Iteration 732, loss = 0.04511236\n",
      "Iteration 733, loss = 0.04497651\n",
      "Iteration 734, loss = 0.04481100\n",
      "Iteration 735, loss = 0.04456678\n",
      "Iteration 736, loss = 0.04441599\n",
      "Iteration 737, loss = 0.04419683\n",
      "Iteration 738, loss = 0.04402365\n",
      "Iteration 739, loss = 0.04382533\n",
      "Iteration 740, loss = 0.04365027\n",
      "Iteration 741, loss = 0.04350221\n",
      "Iteration 742, loss = 0.04330866\n",
      "Iteration 743, loss = 0.04307559\n",
      "Iteration 744, loss = 0.04290625\n",
      "Iteration 745, loss = 0.04272207\n",
      "Iteration 746, loss = 0.04260022\n",
      "Iteration 747, loss = 0.04240750\n",
      "Iteration 748, loss = 0.04220982\n",
      "Iteration 749, loss = 0.04202112\n",
      "Iteration 750, loss = 0.04187744\n",
      "Iteration 751, loss = 0.04168257\n",
      "Iteration 752, loss = 0.04153063\n",
      "Iteration 753, loss = 0.04131360\n",
      "Iteration 754, loss = 0.04115110\n",
      "Iteration 755, loss = 0.04097666\n",
      "Iteration 756, loss = 0.04086106\n",
      "Iteration 757, loss = 0.04064729\n",
      "Iteration 758, loss = 0.04049207\n",
      "Iteration 759, loss = 0.04033934\n",
      "Iteration 760, loss = 0.04014315\n",
      "Iteration 761, loss = 0.03996462\n",
      "Iteration 762, loss = 0.03984603\n",
      "Iteration 763, loss = 0.03967151\n",
      "Iteration 764, loss = 0.03948064\n",
      "Iteration 765, loss = 0.03934382\n",
      "Iteration 766, loss = 0.03922043\n",
      "Iteration 767, loss = 0.03901799\n",
      "Iteration 768, loss = 0.03882970\n",
      "Iteration 769, loss = 0.03869217\n",
      "Iteration 770, loss = 0.03852709\n",
      "Iteration 771, loss = 0.03838647\n",
      "Iteration 772, loss = 0.03820410\n",
      "Iteration 773, loss = 0.03809627\n",
      "Iteration 774, loss = 0.03787474\n",
      "Iteration 775, loss = 0.03775140\n",
      "Iteration 776, loss = 0.03760390\n",
      "Iteration 777, loss = 0.03746040\n",
      "Iteration 778, loss = 0.03730359\n",
      "Iteration 779, loss = 0.03712112\n",
      "Iteration 780, loss = 0.03699203\n",
      "Iteration 781, loss = 0.03682299\n",
      "Iteration 782, loss = 0.03667706\n",
      "Iteration 783, loss = 0.03654332\n",
      "Iteration 784, loss = 0.03644086\n",
      "Iteration 785, loss = 0.03622631\n",
      "Iteration 786, loss = 0.03609932\n",
      "Iteration 787, loss = 0.03595757\n",
      "Iteration 788, loss = 0.03579615\n",
      "Iteration 789, loss = 0.03566246\n",
      "Iteration 790, loss = 0.03553319\n",
      "Iteration 791, loss = 0.03536603\n",
      "Iteration 792, loss = 0.03521406\n",
      "Iteration 793, loss = 0.03507796\n",
      "Iteration 794, loss = 0.03494152\n",
      "Iteration 795, loss = 0.03479966\n",
      "Iteration 796, loss = 0.03464604\n",
      "Iteration 797, loss = 0.03450737\n",
      "Iteration 798, loss = 0.03436119\n",
      "Iteration 799, loss = 0.03421247\n",
      "Iteration 800, loss = 0.03412206\n",
      "Iteration 801, loss = 0.03397691\n",
      "Iteration 802, loss = 0.03386520\n",
      "Iteration 803, loss = 0.03367470\n",
      "Iteration 804, loss = 0.03355234\n",
      "Iteration 805, loss = 0.03339624\n",
      "Iteration 806, loss = 0.03326336\n",
      "Iteration 807, loss = 0.03312592\n",
      "Iteration 808, loss = 0.03299644\n",
      "Iteration 809, loss = 0.03289973\n",
      "Iteration 810, loss = 0.03272138\n",
      "Iteration 811, loss = 0.03262974\n",
      "Iteration 812, loss = 0.03247002\n",
      "Iteration 813, loss = 0.03235703\n",
      "Iteration 814, loss = 0.03223796\n",
      "Iteration 815, loss = 0.03210181\n",
      "Iteration 816, loss = 0.03195302\n",
      "Iteration 817, loss = 0.03181863\n",
      "Iteration 818, loss = 0.03170742\n",
      "Iteration 819, loss = 0.03157535\n",
      "Iteration 820, loss = 0.03143279\n",
      "Iteration 821, loss = 0.03133068\n",
      "Iteration 822, loss = 0.03120359\n",
      "Iteration 823, loss = 0.03109454\n",
      "Iteration 824, loss = 0.03099030\n",
      "Iteration 825, loss = 0.03083843\n",
      "Iteration 826, loss = 0.03072458\n",
      "Iteration 827, loss = 0.03061920\n",
      "Iteration 828, loss = 0.03044962\n",
      "Iteration 829, loss = 0.03033723\n",
      "Iteration 830, loss = 0.03023242\n",
      "Iteration 831, loss = 0.03010558\n",
      "Iteration 832, loss = 0.02999672\n",
      "Iteration 833, loss = 0.02987013\n",
      "Iteration 834, loss = 0.02976224\n",
      "Iteration 835, loss = 0.02962900\n",
      "Iteration 836, loss = 0.02953842\n",
      "Iteration 837, loss = 0.02939213\n",
      "Iteration 838, loss = 0.02928093\n",
      "Iteration 839, loss = 0.02919354\n",
      "Iteration 840, loss = 0.02907884\n",
      "Iteration 841, loss = 0.02894198\n",
      "Iteration 842, loss = 0.02885757\n",
      "Iteration 843, loss = 0.02872563\n",
      "Iteration 844, loss = 0.02860439\n",
      "Iteration 845, loss = 0.02848984\n",
      "Iteration 846, loss = 0.02838354\n",
      "Iteration 847, loss = 0.02828106\n",
      "Iteration 848, loss = 0.02816801\n",
      "Iteration 849, loss = 0.02808772\n",
      "Iteration 850, loss = 0.02793699\n",
      "Iteration 851, loss = 0.02785578\n",
      "Iteration 852, loss = 0.02774114\n",
      "Iteration 853, loss = 0.02762784\n",
      "Iteration 854, loss = 0.02750927\n",
      "Iteration 855, loss = 0.02739051\n",
      "Iteration 856, loss = 0.02728746\n",
      "Iteration 857, loss = 0.02720799\n",
      "Iteration 858, loss = 0.02712351\n",
      "Iteration 859, loss = 0.02699133\n",
      "Iteration 860, loss = 0.02688505\n",
      "Iteration 861, loss = 0.02675770\n",
      "Iteration 862, loss = 0.02668228\n",
      "Iteration 863, loss = 0.02659682\n",
      "Iteration 864, loss = 0.02647000\n",
      "Iteration 865, loss = 0.02636871\n",
      "Iteration 866, loss = 0.02627045\n",
      "Iteration 867, loss = 0.02616150\n",
      "Iteration 868, loss = 0.02605673\n",
      "Iteration 869, loss = 0.02595064\n",
      "Iteration 870, loss = 0.02592038\n",
      "Iteration 871, loss = 0.02576046\n",
      "Iteration 872, loss = 0.02565918\n",
      "Iteration 873, loss = 0.02556262\n",
      "Iteration 874, loss = 0.02548410\n",
      "Iteration 875, loss = 0.02539446\n",
      "Iteration 876, loss = 0.02528956\n",
      "Iteration 877, loss = 0.02520015\n",
      "Iteration 878, loss = 0.02510455\n",
      "Iteration 879, loss = 0.02498340\n",
      "Iteration 880, loss = 0.02490604\n",
      "Iteration 881, loss = 0.02480709\n",
      "Iteration 882, loss = 0.02469661\n",
      "Iteration 883, loss = 0.02461034\n",
      "Iteration 884, loss = 0.02451369\n",
      "Iteration 885, loss = 0.02446014\n",
      "Iteration 886, loss = 0.02435226\n",
      "Iteration 887, loss = 0.02426655\n",
      "Iteration 888, loss = 0.02412694\n",
      "Iteration 889, loss = 0.02405284\n",
      "Iteration 890, loss = 0.02395989\n",
      "Iteration 891, loss = 0.02389332\n",
      "Iteration 892, loss = 0.02376356\n",
      "Iteration 893, loss = 0.02370523\n",
      "Iteration 894, loss = 0.02362351\n",
      "Iteration 895, loss = 0.02349291\n",
      "Iteration 896, loss = 0.02340082\n",
      "Iteration 897, loss = 0.02333021\n",
      "Iteration 898, loss = 0.02324848\n",
      "Iteration 899, loss = 0.02319212\n",
      "Iteration 900, loss = 0.02309586\n",
      "Iteration 901, loss = 0.02298087\n",
      "Iteration 902, loss = 0.02288895\n",
      "Iteration 903, loss = 0.02281459\n",
      "Iteration 904, loss = 0.02273584\n",
      "Iteration 905, loss = 0.02264335\n",
      "Iteration 906, loss = 0.02257254\n",
      "Iteration 907, loss = 0.02247705\n",
      "Iteration 908, loss = 0.02238058\n",
      "Iteration 909, loss = 0.02232088\n",
      "Iteration 910, loss = 0.02227717\n",
      "Iteration 911, loss = 0.02215635\n",
      "Iteration 912, loss = 0.02208525\n",
      "Iteration 913, loss = 0.02199635\n",
      "Iteration 914, loss = 0.02192282\n",
      "Iteration 915, loss = 0.02184238\n",
      "Iteration 916, loss = 0.02175148\n",
      "Iteration 917, loss = 0.02170855\n",
      "Iteration 918, loss = 0.02160830\n",
      "Iteration 919, loss = 0.02151362\n",
      "Iteration 920, loss = 0.02143688\n",
      "Iteration 921, loss = 0.02136567\n",
      "Iteration 922, loss = 0.02128824\n",
      "Iteration 923, loss = 0.02122825\n",
      "Iteration 924, loss = 0.02114555\n",
      "Iteration 925, loss = 0.02107521\n",
      "Iteration 926, loss = 0.02097886\n",
      "Iteration 927, loss = 0.02090774\n",
      "Iteration 928, loss = 0.02083927\n",
      "Iteration 929, loss = 0.02072818\n",
      "Iteration 930, loss = 0.02066849\n",
      "Iteration 931, loss = 0.02058773\n",
      "Iteration 932, loss = 0.02051592\n",
      "Iteration 933, loss = 0.02042866\n",
      "Iteration 934, loss = 0.02038008\n",
      "Iteration 935, loss = 0.02027981\n",
      "Iteration 936, loss = 0.02021665\n",
      "Iteration 937, loss = 0.02013521\n",
      "Iteration 938, loss = 0.02008293\n",
      "Iteration 939, loss = 0.02003043\n",
      "Iteration 940, loss = 0.01994190\n",
      "Iteration 941, loss = 0.01986391\n",
      "Iteration 942, loss = 0.01978265\n",
      "Iteration 943, loss = 0.01974329\n",
      "Iteration 944, loss = 0.01964450\n",
      "Iteration 945, loss = 0.01960821\n",
      "Iteration 946, loss = 0.01951982\n",
      "Iteration 947, loss = 0.01946396\n",
      "Iteration 948, loss = 0.01936196\n",
      "Iteration 949, loss = 0.01931772\n",
      "Iteration 950, loss = 0.01925429\n",
      "Iteration 951, loss = 0.01917083\n",
      "Iteration 952, loss = 0.01910282\n",
      "Iteration 953, loss = 0.01904005\n",
      "Iteration 954, loss = 0.01899473\n",
      "Iteration 955, loss = 0.01890779\n",
      "Iteration 956, loss = 0.01888204\n",
      "Iteration 957, loss = 0.01879441\n",
      "Iteration 958, loss = 0.01870325\n",
      "Iteration 959, loss = 0.01863001\n",
      "Iteration 960, loss = 0.01857595\n",
      "Iteration 961, loss = 0.01850817\n",
      "Iteration 962, loss = 0.01843475\n",
      "Iteration 963, loss = 0.01844252\n",
      "Iteration 964, loss = 0.01829864\n",
      "Iteration 965, loss = 0.01824879\n",
      "Iteration 966, loss = 0.01818050\n",
      "Iteration 967, loss = 0.01812934\n",
      "Iteration 968, loss = 0.01805485\n",
      "Iteration 969, loss = 0.01799921\n",
      "Iteration 970, loss = 0.01795360\n",
      "Iteration 971, loss = 0.01786156\n",
      "Iteration 972, loss = 0.01781939\n",
      "Iteration 973, loss = 0.01776929\n",
      "Iteration 974, loss = 0.01770360\n",
      "Iteration 975, loss = 0.01762588\n",
      "Iteration 976, loss = 0.01756317\n",
      "Iteration 977, loss = 0.01751580\n",
      "Iteration 978, loss = 0.01747344\n",
      "Iteration 979, loss = 0.01738860\n",
      "Iteration 980, loss = 0.01732956\n",
      "Iteration 981, loss = 0.01727154\n",
      "Iteration 982, loss = 0.01722287\n",
      "Iteration 983, loss = 0.01718919\n",
      "Iteration 984, loss = 0.01712662\n",
      "Iteration 985, loss = 0.01705418\n",
      "Iteration 986, loss = 0.01700220\n",
      "Iteration 987, loss = 0.01692704\n",
      "Iteration 988, loss = 0.01688821\n",
      "Iteration 989, loss = 0.01679741\n",
      "Iteration 990, loss = 0.01675640\n",
      "Iteration 991, loss = 0.01672225\n",
      "Iteration 992, loss = 0.01666216\n",
      "Iteration 993, loss = 0.01659877\n",
      "Iteration 994, loss = 0.01654457\n",
      "Iteration 995, loss = 0.01650559\n",
      "Iteration 996, loss = 0.01644118\n",
      "Iteration 997, loss = 0.01637029\n",
      "Iteration 998, loss = 0.01631001\n",
      "Iteration 999, loss = 0.01627287\n",
      "Iteration 1000, loss = 0.01622349\n",
      "Iteration 1001, loss = 0.01620521\n",
      "Iteration 1002, loss = 0.01611190\n",
      "Iteration 1003, loss = 0.01605226\n",
      "Iteration 1004, loss = 0.01601826\n",
      "Iteration 1005, loss = 0.01596562\n",
      "Iteration 1006, loss = 0.01589979\n",
      "Iteration 1007, loss = 0.01587446\n",
      "Iteration 1008, loss = 0.01582751\n",
      "Iteration 1009, loss = 0.01574635\n",
      "Iteration 1010, loss = 0.01570932\n",
      "Iteration 1011, loss = 0.01565207\n",
      "Iteration 1012, loss = 0.01559905\n",
      "Iteration 1013, loss = 0.01554742\n",
      "Iteration 1014, loss = 0.01548113\n",
      "Iteration 1015, loss = 0.01543612\n",
      "Iteration 1016, loss = 0.01538877\n",
      "Iteration 1017, loss = 0.01535373\n",
      "Iteration 1018, loss = 0.01528642\n",
      "Iteration 1019, loss = 0.01526237\n",
      "Iteration 1020, loss = 0.01519336\n",
      "Iteration 1021, loss = 0.01518492\n",
      "Iteration 1022, loss = 0.01513736\n",
      "Iteration 1023, loss = 0.01505996\n",
      "Iteration 1024, loss = 0.01499362\n",
      "Iteration 1025, loss = 0.01496590\n",
      "Iteration 1026, loss = 0.01492221\n",
      "Iteration 1027, loss = 0.01486584\n",
      "Iteration 1028, loss = 0.01481313\n",
      "Iteration 1029, loss = 0.01478213\n",
      "Iteration 1030, loss = 0.01475747\n",
      "Iteration 1031, loss = 0.01466297\n",
      "Iteration 1032, loss = 0.01460830\n",
      "Iteration 1033, loss = 0.01457596\n",
      "Iteration 1034, loss = 0.01454473\n",
      "Iteration 1035, loss = 0.01449543\n",
      "Iteration 1036, loss = 0.01444029\n",
      "Iteration 1037, loss = 0.01439763\n",
      "Iteration 1038, loss = 0.01438902\n",
      "Iteration 1039, loss = 0.01429888\n",
      "Iteration 1040, loss = 0.01426462\n",
      "Iteration 1041, loss = 0.01422256\n",
      "Iteration 1042, loss = 0.01416690\n",
      "Iteration 1043, loss = 0.01413332\n",
      "Iteration 1044, loss = 0.01410560\n",
      "Iteration 1045, loss = 0.01406327\n",
      "Iteration 1046, loss = 0.01400092\n",
      "Iteration 1047, loss = 0.01397990\n",
      "Iteration 1048, loss = 0.01389341\n",
      "Iteration 1049, loss = 0.01386811\n",
      "Iteration 1050, loss = 0.01382898\n",
      "Iteration 1051, loss = 0.01378878\n",
      "Iteration 1052, loss = 0.01377983\n",
      "Iteration 1053, loss = 0.01368080\n",
      "Iteration 1054, loss = 0.01367162\n",
      "Iteration 1055, loss = 0.01365208\n",
      "Iteration 1056, loss = 0.01358660\n",
      "Iteration 1057, loss = 0.01353848\n",
      "Iteration 1058, loss = 0.01350420\n",
      "Iteration 1059, loss = 0.01343864\n",
      "Iteration 1060, loss = 0.01340793\n",
      "Iteration 1061, loss = 0.01337042\n",
      "Iteration 1062, loss = 0.01331644\n",
      "Iteration 1063, loss = 0.01327299\n",
      "Iteration 1064, loss = 0.01327099\n",
      "Iteration 1065, loss = 0.01323670\n",
      "Iteration 1066, loss = 0.01316494\n",
      "Iteration 1067, loss = 0.01313006\n",
      "Iteration 1068, loss = 0.01310653\n",
      "Iteration 1069, loss = 0.01305831\n",
      "Iteration 1070, loss = 0.01301653\n",
      "Iteration 1071, loss = 0.01298415\n",
      "Iteration 1072, loss = 0.01293497\n",
      "Iteration 1073, loss = 0.01290486\n",
      "Iteration 1074, loss = 0.01285936\n",
      "Iteration 1075, loss = 0.01283440\n",
      "Iteration 1076, loss = 0.01280057\n",
      "Iteration 1077, loss = 0.01273872\n",
      "Iteration 1078, loss = 0.01270245\n",
      "Iteration 1079, loss = 0.01270158\n",
      "Iteration 1080, loss = 0.01262557\n",
      "Iteration 1081, loss = 0.01258371\n",
      "Iteration 1082, loss = 0.01257623\n",
      "Iteration 1083, loss = 0.01255377\n",
      "Iteration 1084, loss = 0.01248699\n",
      "Iteration 1085, loss = 0.01244716\n",
      "Iteration 1086, loss = 0.01241373\n",
      "Iteration 1087, loss = 0.01240850\n",
      "Iteration 1088, loss = 0.01237958\n",
      "Iteration 1089, loss = 0.01230796\n",
      "Iteration 1090, loss = 0.01225090\n",
      "Iteration 1091, loss = 0.01223656\n",
      "Iteration 1092, loss = 0.01223527\n",
      "Iteration 1093, loss = 0.01220152\n",
      "Iteration 1094, loss = 0.01219736\n",
      "Iteration 1095, loss = 0.01210210\n",
      "Iteration 1096, loss = 0.01205293\n",
      "Iteration 1097, loss = 0.01203924\n",
      "Iteration 1098, loss = 0.01198246\n",
      "Iteration 1099, loss = 0.01196808\n",
      "Iteration 1100, loss = 0.01193392\n",
      "Iteration 1101, loss = 0.01195216\n",
      "Iteration 1102, loss = 0.01190608\n",
      "Iteration 1103, loss = 0.01183757\n",
      "Iteration 1104, loss = 0.01178402\n",
      "Iteration 1105, loss = 0.01175554\n",
      "Iteration 1106, loss = 0.01172483\n",
      "Iteration 1107, loss = 0.01171436\n",
      "Iteration 1108, loss = 0.01166387\n",
      "Iteration 1109, loss = 0.01163820\n",
      "Iteration 1110, loss = 0.01162800\n",
      "Iteration 1111, loss = 0.01160950\n",
      "Iteration 1112, loss = 0.01153051\n",
      "Iteration 1113, loss = 0.01148804\n",
      "Iteration 1114, loss = 0.01146732\n",
      "Iteration 1115, loss = 0.01142070\n",
      "Iteration 1116, loss = 0.01139955\n",
      "Iteration 1117, loss = 0.01135416\n",
      "Iteration 1118, loss = 0.01134162\n",
      "Iteration 1119, loss = 0.01129780\n",
      "Iteration 1120, loss = 0.01129315\n",
      "Iteration 1121, loss = 0.01123281\n",
      "Iteration 1122, loss = 0.01120539\n",
      "Iteration 1123, loss = 0.01118752\n",
      "Iteration 1124, loss = 0.01119160\n",
      "Iteration 1125, loss = 0.01114419\n",
      "Iteration 1126, loss = 0.01109132\n",
      "Iteration 1127, loss = 0.01107837\n",
      "Iteration 1128, loss = 0.01101455\n",
      "Iteration 1129, loss = 0.01098041\n",
      "Iteration 1130, loss = 0.01095031\n",
      "Iteration 1131, loss = 0.01092633\n",
      "Iteration 1132, loss = 0.01089634\n",
      "Iteration 1133, loss = 0.01086755\n",
      "Iteration 1134, loss = 0.01083018\n",
      "Iteration 1135, loss = 0.01081743\n",
      "Iteration 1136, loss = 0.01079103\n",
      "Iteration 1137, loss = 0.01074366\n",
      "Iteration 1138, loss = 0.01072240\n",
      "Iteration 1139, loss = 0.01067935\n",
      "Iteration 1140, loss = 0.01066487\n",
      "Iteration 1141, loss = 0.01065104\n",
      "Iteration 1142, loss = 0.01060871\n",
      "Iteration 1143, loss = 0.01058036\n",
      "Iteration 1144, loss = 0.01054413\n",
      "Iteration 1145, loss = 0.01050696\n",
      "Iteration 1146, loss = 0.01046833\n",
      "Iteration 1147, loss = 0.01045916\n",
      "Iteration 1148, loss = 0.01046469\n",
      "Iteration 1149, loss = 0.01040448\n",
      "Iteration 1150, loss = 0.01036749\n",
      "Iteration 1151, loss = 0.01034402\n",
      "Iteration 1152, loss = 0.01034000\n",
      "Iteration 1153, loss = 0.01030996\n",
      "Iteration 1154, loss = 0.01026048\n",
      "Iteration 1155, loss = 0.01030678\n",
      "Iteration 1156, loss = 0.01022056\n",
      "Iteration 1157, loss = 0.01018419\n",
      "Iteration 1158, loss = 0.01018149\n",
      "Iteration 1159, loss = 0.01014060\n",
      "Iteration 1160, loss = 0.01017442\n",
      "Iteration 1161, loss = 0.01007260\n",
      "Iteration 1162, loss = 0.01008935\n",
      "Iteration 1163, loss = 0.01001274\n",
      "Iteration 1164, loss = 0.01001487\n",
      "Iteration 1165, loss = 0.00995898\n",
      "Iteration 1166, loss = 0.00993728\n",
      "Iteration 1167, loss = 0.00991908\n",
      "Iteration 1168, loss = 0.00988774\n",
      "Iteration 1169, loss = 0.00990298\n",
      "Iteration 1170, loss = 0.00981092\n",
      "Iteration 1171, loss = 0.00983347\n",
      "Iteration 1172, loss = 0.00978758\n",
      "Iteration 1173, loss = 0.00974827\n",
      "Iteration 1174, loss = 0.00971190\n",
      "Iteration 1175, loss = 0.00969101\n",
      "Iteration 1176, loss = 0.00966310\n",
      "Iteration 1177, loss = 0.00963231\n",
      "Iteration 1178, loss = 0.00962060\n",
      "Iteration 1179, loss = 0.00958028\n",
      "Iteration 1180, loss = 0.00961751\n",
      "Iteration 1181, loss = 0.00958184\n",
      "Iteration 1182, loss = 0.00952718\n",
      "Iteration 1183, loss = 0.00948636\n",
      "Iteration 1184, loss = 0.00946889\n",
      "Iteration 1185, loss = 0.00943914\n",
      "Iteration 1186, loss = 0.00940664\n",
      "Iteration 1187, loss = 0.00938823\n",
      "Iteration 1188, loss = 0.00938495\n",
      "Iteration 1189, loss = 0.00933877\n",
      "Iteration 1190, loss = 0.00932423\n",
      "Iteration 1191, loss = 0.00930841\n",
      "Iteration 1192, loss = 0.00927919\n",
      "Iteration 1193, loss = 0.00923775\n",
      "Iteration 1194, loss = 0.00922359\n",
      "Iteration 1195, loss = 0.00921827\n",
      "Iteration 1196, loss = 0.00918644\n",
      "Iteration 1197, loss = 0.00916867\n",
      "Iteration 1198, loss = 0.00916971\n",
      "Iteration 1199, loss = 0.00912205\n",
      "Iteration 1200, loss = 0.00909247\n",
      "Iteration 1201, loss = 0.00908213\n",
      "Iteration 1202, loss = 0.00904552\n",
      "Iteration 1203, loss = 0.00903527\n",
      "Iteration 1204, loss = 0.00899857\n",
      "Iteration 1205, loss = 0.00900681\n",
      "Iteration 1206, loss = 0.00894161\n",
      "Iteration 1207, loss = 0.00892190\n",
      "Iteration 1208, loss = 0.00891581\n",
      "Iteration 1209, loss = 0.00887397\n",
      "Iteration 1210, loss = 0.00884820\n",
      "Iteration 1211, loss = 0.00883026\n",
      "Iteration 1212, loss = 0.00879643\n",
      "Iteration 1213, loss = 0.00881387\n",
      "Iteration 1214, loss = 0.00875296\n",
      "Iteration 1215, loss = 0.00872834\n",
      "Iteration 1216, loss = 0.00872058\n",
      "Iteration 1217, loss = 0.00870119\n",
      "Iteration 1218, loss = 0.00867009\n",
      "Iteration 1219, loss = 0.00864347\n",
      "Iteration 1220, loss = 0.00863719\n",
      "Iteration 1221, loss = 0.00860310\n",
      "Iteration 1222, loss = 0.00859582\n",
      "Iteration 1223, loss = 0.00855347\n",
      "Iteration 1224, loss = 0.00856832\n",
      "Iteration 1225, loss = 0.00852877\n",
      "Iteration 1226, loss = 0.00850093\n",
      "Iteration 1227, loss = 0.00848307\n",
      "Iteration 1228, loss = 0.00847335\n",
      "Iteration 1229, loss = 0.00843142\n",
      "Iteration 1230, loss = 0.00842897\n",
      "Iteration 1231, loss = 0.00842886\n",
      "Iteration 1232, loss = 0.00836741\n",
      "Iteration 1233, loss = 0.00835958\n",
      "Iteration 1234, loss = 0.00833627\n",
      "Iteration 1235, loss = 0.00829212\n",
      "Iteration 1236, loss = 0.00827861\n",
      "Iteration 1237, loss = 0.00826438\n",
      "Iteration 1238, loss = 0.00826241\n",
      "Iteration 1239, loss = 0.00826980\n",
      "Iteration 1240, loss = 0.00821152\n",
      "Iteration 1241, loss = 0.00820671\n",
      "Iteration 1242, loss = 0.00817735\n",
      "Iteration 1243, loss = 0.00814167\n",
      "Iteration 1244, loss = 0.00814133\n",
      "Iteration 1245, loss = 0.00810820\n",
      "Iteration 1246, loss = 0.00808781\n",
      "Iteration 1247, loss = 0.00805408\n",
      "Iteration 1248, loss = 0.00806623\n",
      "Iteration 1249, loss = 0.00803743\n",
      "Iteration 1250, loss = 0.00799124\n",
      "Iteration 1251, loss = 0.00798096\n",
      "Iteration 1252, loss = 0.00795903\n",
      "Iteration 1253, loss = 0.00794488\n",
      "Iteration 1254, loss = 0.00792654\n",
      "Iteration 1255, loss = 0.00791107\n",
      "Iteration 1256, loss = 0.00788089\n",
      "Iteration 1257, loss = 0.00784483\n",
      "Iteration 1258, loss = 0.00786251\n",
      "Iteration 1259, loss = 0.00784017\n",
      "Iteration 1260, loss = 0.00780142\n",
      "Iteration 1261, loss = 0.00778968\n",
      "Iteration 1262, loss = 0.00776812\n",
      "Iteration 1263, loss = 0.00776630\n",
      "Iteration 1264, loss = 0.00774721\n",
      "Iteration 1265, loss = 0.00770926\n",
      "Iteration 1266, loss = 0.00769342\n",
      "Iteration 1267, loss = 0.00766817\n",
      "Iteration 1268, loss = 0.00765489\n",
      "Iteration 1269, loss = 0.00764117\n",
      "Iteration 1270, loss = 0.00761565\n",
      "Iteration 1271, loss = 0.00760947\n",
      "Iteration 1272, loss = 0.00759013\n",
      "Iteration 1273, loss = 0.00757671\n",
      "Iteration 1274, loss = 0.00753985\n",
      "Iteration 1275, loss = 0.00753429\n",
      "Iteration 1276, loss = 0.00752850\n",
      "Iteration 1277, loss = 0.00755965\n",
      "Iteration 1278, loss = 0.00749063\n",
      "Iteration 1279, loss = 0.00749848\n",
      "Iteration 1280, loss = 0.00746080\n",
      "Iteration 1281, loss = 0.00741506\n",
      "Iteration 1282, loss = 0.00741187\n",
      "Iteration 1283, loss = 0.00740490\n",
      "Iteration 1284, loss = 0.00742060\n",
      "Iteration 1285, loss = 0.00735393\n",
      "Iteration 1286, loss = 0.00735619\n",
      "Iteration 1287, loss = 0.00731164\n",
      "Iteration 1288, loss = 0.00729488\n",
      "Iteration 1289, loss = 0.00727570\n",
      "Iteration 1290, loss = 0.00729838\n",
      "Iteration 1291, loss = 0.00723863\n",
      "Iteration 1292, loss = 0.00724056\n",
      "Iteration 1293, loss = 0.00722786\n",
      "Iteration 1294, loss = 0.00718812\n",
      "Iteration 1295, loss = 0.00716575\n",
      "Iteration 1296, loss = 0.00717401\n",
      "Iteration 1297, loss = 0.00714560\n",
      "Iteration 1298, loss = 0.00711830\n",
      "Iteration 1299, loss = 0.00713213\n",
      "Iteration 1300, loss = 0.00709704\n",
      "Iteration 1301, loss = 0.00709492\n",
      "Iteration 1302, loss = 0.00706559\n",
      "Iteration 1303, loss = 0.00704985\n",
      "Iteration 1304, loss = 0.00703070\n",
      "Iteration 1305, loss = 0.00699998\n",
      "Iteration 1306, loss = 0.00698768\n",
      "Iteration 1307, loss = 0.00698165\n",
      "Iteration 1308, loss = 0.00695975\n",
      "Iteration 1309, loss = 0.00694055\n",
      "Iteration 1310, loss = 0.00692581\n",
      "Iteration 1311, loss = 0.00693191\n",
      "Iteration 1312, loss = 0.00689927\n",
      "Iteration 1313, loss = 0.00688295\n",
      "Iteration 1314, loss = 0.00685444\n",
      "Iteration 1315, loss = 0.00684636\n",
      "Iteration 1316, loss = 0.00681606\n",
      "Iteration 1317, loss = 0.00680465\n",
      "Iteration 1318, loss = 0.00681753\n",
      "Iteration 1319, loss = 0.00678275\n",
      "Iteration 1320, loss = 0.00679180\n",
      "Iteration 1321, loss = 0.00675508\n",
      "Iteration 1322, loss = 0.00674198\n",
      "Iteration 1323, loss = 0.00672512\n",
      "Iteration 1324, loss = 0.00673572\n",
      "Iteration 1325, loss = 0.00669345\n",
      "Iteration 1326, loss = 0.00669009\n",
      "Iteration 1327, loss = 0.00665710\n",
      "Iteration 1328, loss = 0.00664481\n",
      "Iteration 1329, loss = 0.00663835\n",
      "Iteration 1330, loss = 0.00661421\n",
      "Iteration 1331, loss = 0.00661275\n",
      "Iteration 1332, loss = 0.00658655\n",
      "Iteration 1333, loss = 0.00656701\n",
      "Iteration 1334, loss = 0.00657353\n",
      "Iteration 1335, loss = 0.00656568\n",
      "Iteration 1336, loss = 0.00657674\n",
      "Iteration 1337, loss = 0.00659499\n",
      "Iteration 1338, loss = 0.00650933\n",
      "Iteration 1339, loss = 0.00649443\n",
      "Iteration 1340, loss = 0.00646937\n",
      "Iteration 1341, loss = 0.00644994\n",
      "Iteration 1342, loss = 0.00643635\n",
      "Iteration 1343, loss = 0.00641958\n",
      "Iteration 1344, loss = 0.00642274\n",
      "Iteration 1345, loss = 0.00640109\n",
      "Iteration 1346, loss = 0.00637317\n",
      "Iteration 1347, loss = 0.00635665\n",
      "Iteration 1348, loss = 0.00639522\n",
      "Iteration 1349, loss = 0.00635338\n",
      "Iteration 1350, loss = 0.00633843\n",
      "Iteration 1351, loss = 0.00634277\n",
      "Iteration 1352, loss = 0.00631968\n",
      "Iteration 1353, loss = 0.00627265\n",
      "Iteration 1354, loss = 0.00628186\n",
      "Iteration 1355, loss = 0.00626299\n",
      "Iteration 1356, loss = 0.00623824\n",
      "Iteration 1357, loss = 0.00623793\n",
      "Iteration 1358, loss = 0.00619304\n",
      "Iteration 1359, loss = 0.00621099\n",
      "Iteration 1360, loss = 0.00618618\n",
      "Iteration 1361, loss = 0.00615345\n",
      "Iteration 1362, loss = 0.00614013\n",
      "Iteration 1363, loss = 0.00613907\n",
      "Iteration 1364, loss = 0.00611868\n",
      "Iteration 1365, loss = 0.00611933\n",
      "Iteration 1366, loss = 0.00612199\n",
      "Iteration 1367, loss = 0.00607953\n",
      "Iteration 1368, loss = 0.00606680\n",
      "Iteration 1369, loss = 0.00607680\n",
      "Iteration 1370, loss = 0.00605245\n",
      "Iteration 1371, loss = 0.00605779\n",
      "Iteration 1372, loss = 0.00600425\n",
      "Iteration 1373, loss = 0.00599408\n",
      "Iteration 1374, loss = 0.00599597\n",
      "Iteration 1375, loss = 0.00600220\n",
      "Iteration 1376, loss = 0.00596021\n",
      "Iteration 1377, loss = 0.00595145\n",
      "Iteration 1378, loss = 0.00595982\n",
      "Iteration 1379, loss = 0.00595305\n",
      "Iteration 1380, loss = 0.00590128\n",
      "Iteration 1381, loss = 0.00589150\n",
      "Iteration 1382, loss = 0.00588283\n",
      "Iteration 1383, loss = 0.00586993\n",
      "Iteration 1384, loss = 0.00586452\n",
      "Iteration 1385, loss = 0.00584400\n",
      "Iteration 1386, loss = 0.00585370\n",
      "Iteration 1387, loss = 0.00583983\n",
      "Iteration 1388, loss = 0.00580542\n",
      "Iteration 1389, loss = 0.00579728\n",
      "Iteration 1390, loss = 0.00577779\n",
      "Iteration 1391, loss = 0.00577482\n",
      "Iteration 1392, loss = 0.00576199\n",
      "Iteration 1393, loss = 0.00574519\n",
      "Iteration 1394, loss = 0.00572567\n",
      "Iteration 1395, loss = 0.00572340\n",
      "Iteration 1396, loss = 0.00572188\n",
      "Iteration 1397, loss = 0.00570470\n",
      "Iteration 1398, loss = 0.00570740\n",
      "Iteration 1399, loss = 0.00567583\n",
      "Iteration 1400, loss = 0.00567966\n",
      "Iteration 1401, loss = 0.00563040\n",
      "Iteration 1402, loss = 0.00562724\n",
      "Iteration 1403, loss = 0.00561201\n",
      "Iteration 1404, loss = 0.00560966\n",
      "Iteration 1405, loss = 0.00558794\n",
      "Iteration 1406, loss = 0.00561246\n",
      "Iteration 1407, loss = 0.00556232\n",
      "Iteration 1408, loss = 0.00557434\n",
      "Iteration 1409, loss = 0.00554924\n",
      "Iteration 1410, loss = 0.00553239\n",
      "Iteration 1411, loss = 0.00554432\n",
      "Iteration 1412, loss = 0.00550285\n",
      "Iteration 1413, loss = 0.00550338\n",
      "Iteration 1414, loss = 0.00550901\n",
      "Iteration 1415, loss = 0.00548456\n",
      "Iteration 1416, loss = 0.00545984\n",
      "Iteration 1417, loss = 0.00545615\n",
      "Iteration 1418, loss = 0.00544505\n",
      "Iteration 1419, loss = 0.00541471\n",
      "Iteration 1420, loss = 0.00540594\n",
      "Iteration 1421, loss = 0.00541955\n",
      "Iteration 1422, loss = 0.00540914\n",
      "Iteration 1423, loss = 0.00539412\n",
      "Iteration 1424, loss = 0.00539713\n",
      "Iteration 1425, loss = 0.00538991\n",
      "Iteration 1426, loss = 0.00536650\n",
      "Iteration 1427, loss = 0.00534422\n",
      "Iteration 1428, loss = 0.00531772\n",
      "Iteration 1429, loss = 0.00530797\n",
      "Iteration 1430, loss = 0.00532202\n",
      "Iteration 1431, loss = 0.00529847\n",
      "Iteration 1432, loss = 0.00529898\n",
      "Iteration 1433, loss = 0.00527449\n",
      "Iteration 1434, loss = 0.00524822\n",
      "Iteration 1435, loss = 0.00524494\n",
      "Iteration 1436, loss = 0.00524726\n",
      "Iteration 1437, loss = 0.00522235\n",
      "Iteration 1438, loss = 0.00521092\n",
      "Iteration 1439, loss = 0.00519951\n",
      "Iteration 1440, loss = 0.00519127\n",
      "Iteration 1441, loss = 0.00517484\n",
      "Iteration 1442, loss = 0.00518955\n",
      "Iteration 1443, loss = 0.00518122\n",
      "Iteration 1444, loss = 0.00513546\n",
      "Iteration 1445, loss = 0.00516240\n",
      "Iteration 1446, loss = 0.00514901\n",
      "Iteration 1447, loss = 0.00511614\n",
      "Iteration 1448, loss = 0.00510827\n",
      "Iteration 1449, loss = 0.00509540\n",
      "Iteration 1450, loss = 0.00509624\n",
      "Iteration 1451, loss = 0.00508770\n",
      "Iteration 1452, loss = 0.00508281\n",
      "Iteration 1453, loss = 0.00505947\n",
      "Iteration 1454, loss = 0.00506512\n",
      "Iteration 1455, loss = 0.00505130\n",
      "Iteration 1456, loss = 0.00502936\n",
      "Iteration 1457, loss = 0.00502920\n",
      "Iteration 1458, loss = 0.00500515\n",
      "Iteration 1459, loss = 0.00504662\n",
      "Iteration 1460, loss = 0.00502180\n",
      "Iteration 1461, loss = 0.00498859\n",
      "Iteration 1462, loss = 0.00499110\n",
      "Iteration 1463, loss = 0.00495524\n",
      "Iteration 1464, loss = 0.00496549\n",
      "Iteration 1465, loss = 0.00494370\n",
      "Iteration 1466, loss = 0.00491934\n",
      "Iteration 1467, loss = 0.00496368\n",
      "Iteration 1468, loss = 0.00492360\n",
      "Iteration 1469, loss = 0.00491038\n",
      "Iteration 1470, loss = 0.00488381\n",
      "Iteration 1471, loss = 0.00487587\n",
      "Iteration 1472, loss = 0.00486837\n",
      "Iteration 1473, loss = 0.00487087\n",
      "Iteration 1474, loss = 0.00484924\n",
      "Iteration 1475, loss = 0.00484209\n",
      "Iteration 1476, loss = 0.00483521\n",
      "Iteration 1477, loss = 0.00482259\n",
      "Iteration 1478, loss = 0.00481110\n",
      "Iteration 1479, loss = 0.00478544\n",
      "Iteration 1480, loss = 0.00481413\n",
      "Iteration 1481, loss = 0.00478522\n",
      "Iteration 1482, loss = 0.00478795\n",
      "Iteration 1483, loss = 0.00475823\n",
      "Iteration 1484, loss = 0.00475822\n",
      "Iteration 1485, loss = 0.00475181\n",
      "Iteration 1486, loss = 0.00475261\n",
      "Iteration 1487, loss = 0.00474353\n",
      "Iteration 1488, loss = 0.00471351\n",
      "Iteration 1489, loss = 0.00471647\n",
      "Iteration 1490, loss = 0.00470633\n",
      "Iteration 1491, loss = 0.00469459\n",
      "Iteration 1492, loss = 0.00467732\n",
      "Iteration 1493, loss = 0.00468058\n",
      "Iteration 1494, loss = 0.00466047\n",
      "Iteration 1495, loss = 0.00468420\n",
      "Iteration 1496, loss = 0.00464381\n",
      "Iteration 1497, loss = 0.00465124\n",
      "Iteration 1498, loss = 0.00466014\n",
      "Iteration 1499, loss = 0.00463033\n",
      "Iteration 1500, loss = 0.00462032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nunesfi/Desktop/IA/env_ia/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100, solver='adam', activation='relu', hidden_layer_sizes=(2,2))\n",
    "rede_neural_credit.fit(x_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = rede_neural_credit.predict(x_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApIAAAHOCAYAAAArLOl3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAVS0lEQVR4nO3de5SVdbnA8WcLMzhccxDBAEfFe1pC6amTAqswL5CiuczKBDNbouENL2CaHjXttEgzpbSOYnnpVOjBsjwI4qLCzjEvGd7ICwwQoMDAgOLAMOzzhzo2B8WZx5nZAp/PWrPWnt/+7f0+ey0WfHnnnb0LxWKxGAAA0ELblXoAAAC2TEISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFI6tvcBn3jiiSgWi1FWVtbehwYAoBnq6+ujUCjEwIEDN7uv3UOyWCxGfX19LF68uL0PDdAmqqqqSj0CQKtq7gcftntIlpWVxeLFi+Oxz49r70MDtIkRxblv3nqspHMAtJY5c8qbtc81kgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkmyxTpp2S1xWnBs9qvo2ru199Gfja7N/ERetejTOf+XPcdIDt0a/Tw1svP9jo46Ny4pz3/Fr3y8cXoqXAdBs1113Z5SXfzJOPHFCqUeBiIjoWOoBIOPAU74Quw49uMnaPiOHxQl33xB//M5NMXX0hCjv2jk+e8158dXpt8bNA4+NmufnN+6d2OfTmzxn3crath4bIKWmpjZGj748Hnvsuaio6FTqcaBR6ozklClTYvjw4XHAAQfE4MGDY+LEibF+/frWng3eUdc+veJz378oHr3pP5us7/+lEfHSjIfjoW9fHzXPz4+lTzwTvzn1W1HepXPsNWJok72vvbx8k6+G9fXt+CoAmu+uu/47Xn319XjiiTtjhx26l3ocaNTiM5JTp06NSy65JMaPHx/Dhg2LF154IcaPHx+1tbVx5ZVXtsWM0MRRk74dC/74WDx7z/T4l7NOblyf8sVzNtn7Vhxu3NDQXuMBtLrhww+JMWOOjw4dOpR6FGiixWckb7zxxhg+fHiMHj06+vXrF0OHDo2zzz47pkyZEkuWLGmLGaHRfscfEbt95pPxuzGXvefebn17x/AfXx6rqv8Rc+78bTtMB9A2dtutr4jkA6lFIVldXR0LFy6MIUOGNFkfOnRobNy4MWbPnt2qw8E/236HHnHkDZfE9Au+F2sWv/Ku+/YcPjQuXvtknLfoD9Gpe9e45VMnxus1q5rs+cxV58QZz/w+xi2dHaf86Rd+0QYAEloUkvPmzYuIiP79+zdZ79OnT5SVlTXeD23hiB9cHMueeTEe/49fb3bf/If+N24eeGzcedRp0bFTeYyedXt069s7IiI2vF4Xq//xchQ6dIh7T5kQvxx5Zix/5oU4YcoP42Mnj2yHVwEAW48WXSO5evXqiIjo2rVrk/VCoRBdunRpvB9a24DDD419Rg6Lmw485j331q99PVbMfSlWzH0pqv/waJy74KEY/K0x8bszLo+nf3V/PP2r+5vsX/Q/f43KvXaNT190Wjz586lt9AoAYOvTopAsFArv637I+sgXj4zyrp1j7N+nvb345p+3s154IObP+ks8+qO7onbBklj86JzGLfWvrY2aFxZE5Z5Vm33+V/42NwaddkKbzA4AW6sWhWT37m+85cCaNWuarBeLxXjttdeiR48erTcZ/JOHLvlB/Pn7k5us9T3ogDhm8jVx51HfiJrnq+Mr9/80ahcsjjsOP7VxT8ftO8UOA/rHc/dMj4iIfz3/1NiuY4f403d/0uS5dv7E/rHypYVt/0IAYCvSopAcMGBAREQsWLAgBg0a1Li+aNGiqK+vb7wfWtuaxa9s8gs2nXfcISIiVvx9ftRW/yP+dM3NMfJn/x6fvfq8+OvPpkbH7ctj8KVnRKduXeKRSXdGxBs/9j7ih5dEYbvt4ulf3R8dK7aPg874cvT/1MC456QL2v11ATRHTU1trH/z7cwaGjZGXd36WLp0eURE9OjRNSoqti/leGzDWhSS/fr1iz322CNmzpwZI0eObFyfOXNmlJWVxSGHHNLa80GzPfnzqbFh3fr45Lmj46AzvxIN6+tj6ZPPxc8+MzpefvK5iIj4y4/uig3r1sdBZ3w5Pj3+G9Gwbn288vTzcccRX48Xp/2xxK8A4J0dd9wFMWvW443fL1r0ctx776yIiJg8+bIYPfrzpRqNbVyhWCwWW/KA6dOnx9ixY+PCCy+Mww8/PJ599tmYMGFCHH/88XHRRRe95+PnzJkT1dXV8djnx6WHBvgguaw4981bj5V0DoDWMmdOeUREHHDAAZvd1+JPtjnssMNi4sSJcdNNN8W1114bO+64Y4waNSrGjBmTmxQAgC1Si0MyImLEiBExYsSI1p4FAIAtSIs/IhEAACKEJAAASUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFI6lurA1++wrFSHBmhVlzXe+ngJpwBoTXOatcsZSYD3qbKystQjAJRESc5IVlVVRU1NTSkODdDqKisro7KyMmpeuK7UowC0iurqnlFVVfWe+5yRBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUiyVVqyZEk88sgjMWvWrHj44YfjxRdfjI0bN5Z6LIBmm79gWYw86frotsvpUTngzBh50vUxf8Gyd9z7ne//Jgo9R8dtd/2xnadkWyck2eosXbo05s6dGzvvvHMcfPDBsffee8fSpUvj+eefL/VoAM1Su3ptDD36u9Gje+f4y4xvx7Rfj4tFi1fGkSdcu8l/ip+duzi+e/3vSzQp27pUSN52222x//77x7nnntva88D7Nn/+/Nhpp52if//+UVFRET179oxdd901lixZEnV1daUeD+A9/fAn06Nb14qYfOOpsc9eH46DBu0ev/jp6XHlxcfFunUbGvdt3Lgxvn7OrXHKlw8p4bRsy1oUkqtWrYrTTz89brnllujUqVNbzQRpa9eujbq6uujZs2eT9be+X7lyZSnGAmiRKb95NE489uDYbru3/5nec0CfOP7og6Kiorxx7YafzogFi2riqou/UIoxoWUhed9998XatWtj6tSp0aNHj7aaCdJef/31iIjYfvvtm6x36tQpCoVCrF27thRjATRbff2GeGbu4uj34coYe9HtsctHz4ve+5wVXzrtx7F4ydv/GZ6/YFl86zt3x03fPzm6d68o4cRsy1oUkkOGDInJkydvcrYHPig2bHjjRz4dO3Zssl4oFKJDhw6N9wN8UNWsfC02bGiIS6+5J3bs2S3uvePsmPS9r8YfHp4bw790XeM1kt8497Y45siBMfxzB5Z2YLZpHd97y9v69+/fVnMAABFRX98QERGHfmqvuOzCkRERMfCjVVFe1jGOOen6+P30v8Ury1fHX59aEM88fHUJJ4UWhiR80L11JvL/n3ksFovR0NAQZWVlpRgLoNm6d3vjx9SfOHC3JutDPr13REQ89uT8uP7m6XHztaNix57d2n0++Gfe/oetSufOnSPi7Wsl31JXVxfFYrHxfoAPqu7dK6L3Tt1jRc2rTdY3biw23l656rX40mk3Rcedvtb4FRFx6tm3Nt6G9uCMJFuVioqK6Ny5c6xYsSL69OnTuL5ixYooFApRWVlZwukAmueoYR+Lqb9/PK68+LgoFAoREfGHh+dGRMRH9ukbc/501SaPOeCQS+KK8cfGMUcNatdZ2bYJSbY6u+22Wzz99NOxcOHC6NWrV6xZsybmz58f/fr1i/Ly8vd+AoASG3/28Pj4Zy6PU8+6NcadeUQsWlwTY8ffEf968B5x/NEHvevj+u68Q+y/b7/2G5RtnpBkq9OrV6/Yd999o7q6Ol566aUoLy+Pfv36RVVVValHA2iWvfboEw/+14VxweW/jIOG/Vt0Ku8YRx320bjuqi+XejRookUhuWrVqqivr4+IiIaGhli3bl0sW/bG535269Ztk/fug1Lp3bt39O7du9RjAKQd/PHdY9ZvJzR7f3HFbW03DLyLFoXk2LFj45FHHmn8funSpfHggw9GRMQ111wTxx13XOtOBwDAB1aLQvL2229vqzkAANjCePsfAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABShCQAAClCEgCAFCEJAECKkAQAIEVIAgCQIiQBAEgRkgAApAhJAABSCsVisdieB3z88cejWCxGeXl5ex4WoM1UV1eXegSAVtWrV68oKyuLQYMGbXZfx3aap1GhUGjvQwK0qaqqqlKPANCq6uvrm9Vs7X5GEgCArYNrJAEASBGSAACkCEkAAFKEJAAAKUISAIAUIQkAQIqQBAAgRUgCAJAiJAEASGn3j0iEtvDyyy/H7NmzY968eVFbWxuFQiE+9KEPxe677x6HHnpoVFZWlnpEANjqCEm2aOvXr4+rrroq7r777mhoaIiysrLo0qVLRES8+uqrsWHDhigrK4tRo0bFuHHjfNY7sFVZt25d3H///TFy5MhSj8I2SkiyRbvhhhti+vTpcemll8bQoUOjT58+Te5ftGhRPPjggzFp0qTo0qVLjBkzpkSTArS+NWvWxIQJE4QkJVMoFovFUg8BWUOGDIkrrrgihgwZstl9M2bMiKuvvjpmzpzZTpMBtL3ly5fHoYceGs8++2ypR2Eb5YwkW7SVK1fGnnvu+Z779ttvv1i+fHk7TATw/o0bN65Z+9atW9fGk8DmCUm2aLvsskvMmDEjTj755M3umzZtWlRVVbXTVADvz7Rp06KioiK6deu22X0bN25sp4ngnQlJtmijR4+OSy+9NJ566qkYPHhw9O/fP7p27RoRb1w7VF1dHQ899FA88MADMXHixBJPC9A8559/fkyePDmmTJmy2XedWLZsWQwePLgdJ4OmXCPJFm/q1KkxadKkWLhw4Sa/lV0sFmOPPfaIc845J4YNG1aiCQFa7vTTT4+6urqYPHnyu77jhGskKTUhyVZjwYIFMW/evFi9enUUCoXo3r17DBgwIPr27Vvq0QBarLa2Nu67774YOnTou/49VltbG9/85jfj9ttvb+fp4A1CEgCAFB+RCABAipAEACBFSAIAkCIkAQBIEZIAAKQISQAAUoQkAAApQhIAgJT/A2toBbGfdcqcAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(x_credit_treinamento,y_credit_treinamento)\n",
    "cm.score(x_credit_teste,y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASE CENSUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nunesfi/Desktop/IA/files/Bases de dados-20230823T013645Z-001/Bases de dados/census.pkl', 'rb') as f:\n",
    "    x_census_treinamento, y_census_treinamento, x_census_teste, y_census_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39376408\n",
      "Iteration 2, loss = 0.32597083\n",
      "Iteration 3, loss = 0.31486255\n",
      "Iteration 4, loss = 0.30764436\n",
      "Iteration 5, loss = 0.30221222\n",
      "Iteration 6, loss = 0.29860664\n",
      "Iteration 7, loss = 0.29573037\n",
      "Iteration 8, loss = 0.29247859\n",
      "Iteration 9, loss = 0.29077198\n",
      "Iteration 10, loss = 0.28858014\n",
      "Iteration 11, loss = 0.28599510\n",
      "Iteration 12, loss = 0.28430395\n",
      "Iteration 13, loss = 0.28149232\n",
      "Iteration 14, loss = 0.27997361\n",
      "Iteration 15, loss = 0.27929519\n",
      "Iteration 16, loss = 0.27668288\n",
      "Iteration 17, loss = 0.27541542\n",
      "Iteration 18, loss = 0.27466274\n",
      "Iteration 19, loss = 0.27333221\n",
      "Iteration 20, loss = 0.27101859\n",
      "Iteration 21, loss = 0.26927468\n",
      "Iteration 22, loss = 0.26801622\n",
      "Iteration 23, loss = 0.26579066\n",
      "Iteration 24, loss = 0.26546363\n",
      "Iteration 25, loss = 0.26330438\n",
      "Iteration 26, loss = 0.26281450\n",
      "Iteration 27, loss = 0.26058829\n",
      "Iteration 28, loss = 0.26024482\n",
      "Iteration 29, loss = 0.25816688\n",
      "Iteration 30, loss = 0.25773265\n",
      "Iteration 31, loss = 0.25542987\n",
      "Iteration 32, loss = 0.25341961\n",
      "Iteration 33, loss = 0.25269367\n",
      "Iteration 34, loss = 0.25092105\n",
      "Iteration 35, loss = 0.25023448\n",
      "Iteration 36, loss = 0.24829095\n",
      "Iteration 37, loss = 0.24769271\n",
      "Iteration 38, loss = 0.24671877\n",
      "Iteration 39, loss = 0.24593417\n",
      "Iteration 40, loss = 0.24427094\n",
      "Iteration 41, loss = 0.24385225\n",
      "Iteration 42, loss = 0.24274217\n",
      "Iteration 43, loss = 0.24112581\n",
      "Iteration 44, loss = 0.24129566\n",
      "Iteration 45, loss = 0.23885420\n",
      "Iteration 46, loss = 0.23737727\n",
      "Iteration 47, loss = 0.23724018\n",
      "Iteration 48, loss = 0.23565042\n",
      "Iteration 49, loss = 0.23516363\n",
      "Iteration 50, loss = 0.23387264\n",
      "Iteration 51, loss = 0.23472677\n",
      "Iteration 52, loss = 0.23253074\n",
      "Iteration 53, loss = 0.23164970\n",
      "Iteration 54, loss = 0.23173980\n",
      "Iteration 55, loss = 0.22988931\n",
      "Iteration 56, loss = 0.22867539\n",
      "Iteration 57, loss = 0.22778183\n",
      "Iteration 58, loss = 0.22759740\n",
      "Iteration 59, loss = 0.22583689\n",
      "Iteration 60, loss = 0.22445304\n",
      "Iteration 61, loss = 0.22475735\n",
      "Iteration 62, loss = 0.22317448\n",
      "Iteration 63, loss = 0.22303324\n",
      "Iteration 64, loss = 0.22366694\n",
      "Iteration 65, loss = 0.22130591\n",
      "Iteration 66, loss = 0.22006850\n",
      "Iteration 67, loss = 0.22013450\n",
      "Iteration 68, loss = 0.21844454\n",
      "Iteration 69, loss = 0.21905007\n",
      "Iteration 70, loss = 0.21738774\n",
      "Iteration 71, loss = 0.21716408\n",
      "Iteration 72, loss = 0.21577331\n",
      "Iteration 73, loss = 0.21497516\n",
      "Iteration 74, loss = 0.21402558\n",
      "Iteration 75, loss = 0.21389172\n",
      "Iteration 76, loss = 0.21245565\n",
      "Iteration 77, loss = 0.21342224\n",
      "Iteration 78, loss = 0.21173661\n",
      "Iteration 79, loss = 0.21112455\n",
      "Iteration 80, loss = 0.21056943\n",
      "Iteration 81, loss = 0.20923044\n",
      "Iteration 82, loss = 0.20942449\n",
      "Iteration 83, loss = 0.20904980\n",
      "Iteration 84, loss = 0.20781018\n",
      "Iteration 85, loss = 0.20752712\n",
      "Iteration 86, loss = 0.20676985\n",
      "Iteration 87, loss = 0.20560688\n",
      "Iteration 88, loss = 0.20551390\n",
      "Iteration 89, loss = 0.20524756\n",
      "Iteration 90, loss = 0.20472310\n",
      "Iteration 91, loss = 0.20416517\n",
      "Iteration 92, loss = 0.20188384\n",
      "Iteration 93, loss = 0.20185957\n",
      "Iteration 94, loss = 0.20181804\n",
      "Iteration 95, loss = 0.20189753\n",
      "Iteration 96, loss = 0.20272082\n",
      "Iteration 97, loss = 0.20052756\n",
      "Iteration 98, loss = 0.20134198\n",
      "Iteration 99, loss = 0.19876766\n",
      "Iteration 100, loss = 0.19851636\n",
      "Iteration 101, loss = 0.19834934\n",
      "Iteration 102, loss = 0.19749972\n",
      "Iteration 103, loss = 0.19751428\n",
      "Iteration 104, loss = 0.19623889\n",
      "Iteration 105, loss = 0.19738018\n",
      "Iteration 106, loss = 0.19635314\n",
      "Iteration 107, loss = 0.19586780\n",
      "Iteration 108, loss = 0.19561008\n",
      "Iteration 109, loss = 0.19531224\n",
      "Iteration 110, loss = 0.19417092\n",
      "Iteration 111, loss = 0.19391590\n",
      "Iteration 112, loss = 0.19276392\n",
      "Iteration 113, loss = 0.19290458\n",
      "Iteration 114, loss = 0.19218609\n",
      "Iteration 115, loss = 0.19306024\n",
      "Iteration 116, loss = 0.19169375\n",
      "Iteration 117, loss = 0.18976646\n",
      "Iteration 118, loss = 0.19087717\n",
      "Iteration 119, loss = 0.19143808\n",
      "Iteration 120, loss = 0.19005135\n",
      "Iteration 121, loss = 0.18945590\n",
      "Iteration 122, loss = 0.18954849\n",
      "Iteration 123, loss = 0.18795297\n",
      "Iteration 124, loss = 0.18934087\n",
      "Iteration 125, loss = 0.18804583\n",
      "Iteration 126, loss = 0.18702016\n",
      "Iteration 127, loss = 0.18780872\n",
      "Iteration 128, loss = 0.18668560\n",
      "Iteration 129, loss = 0.18604312\n",
      "Iteration 130, loss = 0.18592459\n",
      "Iteration 131, loss = 0.18514175\n",
      "Iteration 132, loss = 0.18555070\n",
      "Iteration 133, loss = 0.18596322\n",
      "Iteration 134, loss = 0.18507034\n",
      "Iteration 135, loss = 0.18570506\n",
      "Iteration 136, loss = 0.18392957\n",
      "Iteration 137, loss = 0.18268261\n",
      "Iteration 138, loss = 0.18326671\n",
      "Iteration 139, loss = 0.18216623\n",
      "Iteration 140, loss = 0.18179982\n",
      "Iteration 141, loss = 0.18166295\n",
      "Iteration 142, loss = 0.18182690\n",
      "Iteration 143, loss = 0.18180520\n",
      "Iteration 144, loss = 0.18171876\n",
      "Iteration 145, loss = 0.18096365\n",
      "Iteration 146, loss = 0.18221386\n",
      "Iteration 147, loss = 0.18047959\n",
      "Iteration 148, loss = 0.17965859\n",
      "Iteration 149, loss = 0.17946525\n",
      "Iteration 150, loss = 0.17954829\n",
      "Iteration 151, loss = 0.17804919\n",
      "Iteration 152, loss = 0.17962211\n",
      "Iteration 153, loss = 0.17905659\n",
      "Iteration 154, loss = 0.17925461\n",
      "Iteration 155, loss = 0.17768720\n",
      "Iteration 156, loss = 0.17727332\n",
      "Iteration 157, loss = 0.17771458\n",
      "Iteration 158, loss = 0.17689343\n",
      "Iteration 159, loss = 0.17664791\n",
      "Iteration 160, loss = 0.17641585\n",
      "Iteration 161, loss = 0.17693281\n",
      "Iteration 162, loss = 0.17717196\n",
      "Iteration 163, loss = 0.17525773\n",
      "Iteration 164, loss = 0.17601642\n",
      "Iteration 165, loss = 0.17517449\n",
      "Iteration 166, loss = 0.17455207\n",
      "Iteration 167, loss = 0.17407700\n",
      "Iteration 168, loss = 0.17368957\n",
      "Iteration 169, loss = 0.17566891\n",
      "Iteration 170, loss = 0.17301431\n",
      "Iteration 171, loss = 0.17390515\n",
      "Iteration 172, loss = 0.17254692\n",
      "Iteration 173, loss = 0.17290649\n",
      "Iteration 174, loss = 0.17406021\n",
      "Iteration 175, loss = 0.17480430\n",
      "Iteration 176, loss = 0.17034277\n",
      "Iteration 177, loss = 0.17203812\n",
      "Iteration 178, loss = 0.17097700\n",
      "Iteration 179, loss = 0.17155814\n",
      "Iteration 180, loss = 0.16961651\n",
      "Iteration 181, loss = 0.17114423\n",
      "Iteration 182, loss = 0.17080456\n",
      "Iteration 183, loss = 0.17026010\n",
      "Iteration 184, loss = 0.17039373\n",
      "Iteration 185, loss = 0.16900673\n",
      "Iteration 186, loss = 0.17015675\n",
      "Iteration 187, loss = 0.16908742\n",
      "Iteration 188, loss = 0.16999034\n",
      "Iteration 189, loss = 0.16875533\n",
      "Iteration 190, loss = 0.17050560\n",
      "Iteration 191, loss = 0.16934908\n",
      "Iteration 192, loss = 0.16738611\n",
      "Iteration 193, loss = 0.16640865\n",
      "Iteration 194, loss = 0.16688668\n",
      "Iteration 195, loss = 0.16704448\n",
      "Iteration 196, loss = 0.16679743\n",
      "Iteration 197, loss = 0.16696638\n",
      "Iteration 198, loss = 0.16704934\n",
      "Iteration 199, loss = 0.16661525\n",
      "Iteration 200, loss = 0.16724602\n",
      "Iteration 201, loss = 0.16494453\n",
      "Iteration 202, loss = 0.16453145\n",
      "Iteration 203, loss = 0.16695893\n",
      "Iteration 204, loss = 0.16580316\n",
      "Iteration 205, loss = 0.16432087\n",
      "Iteration 206, loss = 0.16516739\n",
      "Iteration 207, loss = 0.16589826\n",
      "Iteration 208, loss = 0.16343127\n",
      "Iteration 209, loss = 0.16555115\n",
      "Iteration 210, loss = 0.16412689\n",
      "Iteration 211, loss = 0.16256769\n",
      "Iteration 212, loss = 0.16300281\n",
      "Iteration 213, loss = 0.16282585\n",
      "Iteration 214, loss = 0.16225492\n",
      "Iteration 215, loss = 0.16218238\n",
      "Iteration 216, loss = 0.16191285\n",
      "Iteration 217, loss = 0.16201872\n",
      "Iteration 218, loss = 0.16269074\n",
      "Iteration 219, loss = 0.16188092\n",
      "Iteration 220, loss = 0.16225627\n",
      "Iteration 221, loss = 0.16082518\n",
      "Iteration 222, loss = 0.16050525\n",
      "Iteration 223, loss = 0.16056064\n",
      "Iteration 224, loss = 0.16204622\n",
      "Iteration 225, loss = 0.16174227\n",
      "Iteration 226, loss = 0.16039572\n",
      "Iteration 227, loss = 0.15938632\n",
      "Iteration 228, loss = 0.16082157\n",
      "Iteration 229, loss = 0.15960086\n",
      "Iteration 230, loss = 0.16043794\n",
      "Iteration 231, loss = 0.15963469\n",
      "Iteration 232, loss = 0.15905174\n",
      "Iteration 233, loss = 0.15855114\n",
      "Iteration 234, loss = 0.15788113\n",
      "Iteration 235, loss = 0.15840149\n",
      "Iteration 236, loss = 0.15806286\n",
      "Iteration 237, loss = 0.15765299\n",
      "Iteration 238, loss = 0.15909727\n",
      "Iteration 239, loss = 0.15764651\n",
      "Iteration 240, loss = 0.15687723\n",
      "Iteration 241, loss = 0.15783539\n",
      "Iteration 242, loss = 0.15753339\n",
      "Iteration 243, loss = 0.15702607\n",
      "Iteration 244, loss = 0.15881252\n",
      "Iteration 245, loss = 0.15561098\n",
      "Iteration 246, loss = 0.15715773\n",
      "Iteration 247, loss = 0.15709348\n",
      "Iteration 248, loss = 0.15680290\n",
      "Iteration 249, loss = 0.15648064\n",
      "Iteration 250, loss = 0.15479275\n",
      "Iteration 251, loss = 0.15545157\n",
      "Iteration 252, loss = 0.15663256\n",
      "Iteration 253, loss = 0.15542921\n",
      "Iteration 254, loss = 0.15765232\n",
      "Iteration 255, loss = 0.15615008\n",
      "Iteration 256, loss = 0.15607750\n",
      "Iteration 257, loss = 0.15476462\n",
      "Iteration 258, loss = 0.15337351\n",
      "Iteration 259, loss = 0.15473746\n",
      "Iteration 260, loss = 0.15471502\n",
      "Iteration 261, loss = 0.15470512\n",
      "Iteration 262, loss = 0.15326202\n",
      "Iteration 263, loss = 0.15552483\n",
      "Iteration 264, loss = 0.15260816\n",
      "Iteration 265, loss = 0.15335104\n",
      "Iteration 266, loss = 0.15258862\n",
      "Iteration 267, loss = 0.15187000\n",
      "Iteration 268, loss = 0.15128569\n",
      "Iteration 269, loss = 0.15298913\n",
      "Iteration 270, loss = 0.15187513\n",
      "Iteration 271, loss = 0.15200299\n",
      "Iteration 272, loss = 0.15238544\n",
      "Iteration 273, loss = 0.15396998\n",
      "Iteration 274, loss = 0.15254798\n",
      "Iteration 275, loss = 0.15178836\n",
      "Iteration 276, loss = 0.15086945\n",
      "Iteration 277, loss = 0.15112709\n",
      "Iteration 278, loss = 0.15141784\n",
      "Iteration 279, loss = 0.15117906\n",
      "Iteration 280, loss = 0.15038569\n",
      "Iteration 281, loss = 0.15089116\n",
      "Iteration 282, loss = 0.15076445\n",
      "Iteration 283, loss = 0.15145560\n",
      "Iteration 284, loss = 0.15203090\n",
      "Iteration 285, loss = 0.15007237\n",
      "Iteration 286, loss = 0.14991779\n",
      "Iteration 287, loss = 0.15300315\n",
      "Iteration 288, loss = 0.15253973\n",
      "Iteration 289, loss = 0.14934700\n",
      "Iteration 290, loss = 0.14997135\n",
      "Iteration 291, loss = 0.14942989\n",
      "Iteration 292, loss = 0.14904413\n",
      "Iteration 293, loss = 0.14772618\n",
      "Iteration 294, loss = 0.14815481\n",
      "Iteration 295, loss = 0.14869288\n",
      "Iteration 296, loss = 0.14768175\n",
      "Iteration 297, loss = 0.14824799\n",
      "Iteration 298, loss = 0.14932184\n",
      "Iteration 299, loss = 0.14841695\n",
      "Iteration 300, loss = 0.14677512\n",
      "Iteration 301, loss = 0.14743411\n",
      "Iteration 302, loss = 0.14738487\n",
      "Iteration 303, loss = 0.14717917\n",
      "Iteration 304, loss = 0.14796762\n",
      "Iteration 305, loss = 0.14617038\n",
      "Iteration 306, loss = 0.14741162\n",
      "Iteration 307, loss = 0.14718349\n",
      "Iteration 308, loss = 0.14628113\n",
      "Iteration 309, loss = 0.14658312\n",
      "Iteration 310, loss = 0.14820112\n",
      "Iteration 311, loss = 0.14660508\n",
      "Iteration 312, loss = 0.14592219\n",
      "Iteration 313, loss = 0.14580611\n",
      "Iteration 314, loss = 0.14618872\n",
      "Iteration 315, loss = 0.14549037\n",
      "Iteration 316, loss = 0.14604590\n",
      "Iteration 317, loss = 0.14441278\n",
      "Iteration 318, loss = 0.14522483\n",
      "Iteration 319, loss = 0.14570609\n",
      "Iteration 320, loss = 0.14428883\n",
      "Iteration 321, loss = 0.14462143\n",
      "Iteration 322, loss = 0.14417234\n",
      "Iteration 323, loss = 0.14423232\n",
      "Iteration 324, loss = 0.14362876\n",
      "Iteration 325, loss = 0.14621568\n",
      "Iteration 326, loss = 0.14613048\n",
      "Iteration 327, loss = 0.14321104\n",
      "Iteration 328, loss = 0.14380739\n",
      "Iteration 329, loss = 0.14396968\n",
      "Iteration 330, loss = 0.14415246\n",
      "Iteration 331, loss = 0.14300559\n",
      "Iteration 332, loss = 0.14462948\n",
      "Iteration 333, loss = 0.14304374\n",
      "Iteration 334, loss = 0.14312992\n",
      "Iteration 335, loss = 0.14495187\n",
      "Iteration 336, loss = 0.14228209\n",
      "Iteration 337, loss = 0.14134862\n",
      "Iteration 338, loss = 0.14571098\n",
      "Iteration 339, loss = 0.14482608\n",
      "Iteration 340, loss = 0.14211353\n",
      "Iteration 341, loss = 0.14176661\n",
      "Iteration 342, loss = 0.14166560\n",
      "Iteration 343, loss = 0.14186489\n",
      "Iteration 344, loss = 0.14071974\n",
      "Iteration 345, loss = 0.14014294\n",
      "Iteration 346, loss = 0.14242673\n",
      "Iteration 347, loss = 0.13924644\n",
      "Iteration 348, loss = 0.14123696\n",
      "Iteration 349, loss = 0.14139583\n",
      "Iteration 350, loss = 0.14112528\n",
      "Iteration 351, loss = 0.14061183\n",
      "Iteration 352, loss = 0.14135571\n",
      "Iteration 353, loss = 0.13995444\n",
      "Iteration 354, loss = 0.14077382\n",
      "Iteration 355, loss = 0.14055404\n",
      "Iteration 356, loss = 0.14081553\n",
      "Iteration 357, loss = 0.14025528\n",
      "Iteration 358, loss = 0.14097535\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_census  = MLPClassifier(verbose=True, max_iter=1000, tol=0.0000100, hidden_layer_sizes=(55,55))\n",
    "rede_neural_census.fit(x_census_treinamento,y_census_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_census.predict(x_census_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8159672466734903"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_census_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8159672466734903"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAH6CAYAAAAOZCSsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAq3UlEQVR4nO3debRVdf3/8ddlFFBGERUFBRTRcMCpzAHHMmec0BxQRE0wB3AuU+mXE1KmqSnOoKhlOUTWF2cRZxHS1ARBUiwFZFQBvb8/rFs3NFHhHvPzeKx11+Lu/TnnvHfLDs+72Wffqurq6uoAAEAB6lV6AAAAqCviFwCAYohfAACKIX4BACiG+AUAoBjiFwCAYohfAACK0aDSA3zZPfvss6murk7Dhg0rPQoAAB9j4cKFqaqqykYbbfSpa8Xvp6iurs7ChQvzxhtvVHoUgKWiY8eOlR4BYKn6LL+zTfx+ioYNG+aNN97I07sNrPQoAEvFrtUvffSHGddXdhCApWTC6z2WeK1rfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4haWoql69bDGob455YVROnzcux7/2QPa8/vw0X23lmjWrb9EjB4++LifPeCKD/j423/39sLTbYJ1az7P8Kitlr+FDctLbj+X0+c+l79hb0nGbzT7zawEsazvtfWGq2vTJ5Nfeqtk29slXsvWuP0mz1Y9Mq07HZP++l+WNaTNrPe71N2am9xGXpXXn/mm2+pHZapefZOyTr9T1+BRI/MJStNNFp2brM/vn4f93RS5bb9fcefjpWf2bPXLQH65OvQYN0n6z9XPIvddl1uTXc80WvTP8W31Tv1HDHHLvdWnWbsUkSYMmy6XPgzdm1Y3Xy237HJdhm++X2a//LQfdMyzt1u+6xK8FsKxdM+KhPDDmxVrbXvrLtOzQ64Ks2aFtnrnv7Pzfr0/Kq1Peyk77DMnChYuSJAsWLMqOe1+Yia++lXtuHZhxD5yTtTq1y457X5i/THyzEodCQb5U8XvwwQena9eui31ttNFGtdZNnDgx/fr1S48ePbLRRhvliCOOyMSJE2ut6dq1a4YMGbLYa9x0003p2rVrbr755mV6LJSnXoMG6dZrxzx6wbBMGHFn3pn810wa/Wge+NElabtul6zUfe18/YQ+mf3633Jnvx/k7Rcn5c1nX8idR5yRpm1apetu2yZJvtZ7l7RZa43ccdhpmfzA4/n7hJdy+4ED8947s7PVGd9b4tcCWJamvflOBv5wZI7us22t7ef/fFRWbL1Crv754em61irZZKM1c8Nl/fL8i6/ntjueTJKMvP3x/PnlNzLil0dls407Za3OK+fKn/ZJqxbNcuGlv6/E4VCQL93poZ133jlnnHFGrW316v2r0WfMmJFDDjkk3bp1y0033ZR69eplyJAhOfTQQ3P33XenZcuWn/jco0aNyuDBgzNw4MAccMABy+oQKNSHixblZx23XWz7BwsW/mP/B7mz7xlp2KxJUl1ds3/um28nSVZYdaUkyaobr5eF776Xvz42rtZz/GXUQ+nWa8clfi2AZan/yTdmq2+snV67bpxLrhpds/2P9/8pO+/QPQ0a1K/Zts7aq6bTGm1zz70TcuA+38gf7/9TunRql7W7/OsyrQYN6menbdfLPfdOqNPjoDzL5MzvBx98kNGjR+eOO+74zI9dbrnl0rZt21pfbdq0qdl/0003Zd68eRk6dGjWWWedrL322rnwwgszZ86cjBw58hOfd8yYMTn55JNz+OGH58gjj/xcxwWfSVVVVt6wW3qeNSAv331//j7hpSyc/27mvzWj1rKuu2+XJJk6dlySj8K1+sMPF3u6eX97O8u1bJ6mK7ZaotcCWFZuu+OJ3PfwC7n8wkNrbZ837/28Pm1mOnVcabHHdF5jpbz0ykeXNLz0ypvp1LHtx66Z+vqMzJ///rIZHLKU43fmzJm56qqrsuOOO+aMM85I/fr1P/1Bn9GYMWOy0UYbpXnz5jXbWrRokQ033DAPP/zwxz5m/PjxGTBgQPbaa6+cdNJJS30m+E87nDcoP3hvfPo99eu8et9jGbln/49d16Jj+3zn0jMzafSjmfR/Y5Ikb7/0aho1a5qVvlb70oW2X1srSdK4+fKf67UAloYZM+fm2FOH58Kz90/7VWv/MP7OrPlJkuYrLLfY45qv0CQzZ837aN3s+Z+45t+fB5aFpXLZwwsvvJDhw4fn7rvvzpprrpnvfe972X333dO4ceMkyS677JI33njjEx9/1VVXZZNNNlmi13r11Vez0047Lba9Q4cOuffeexfbPnHixBx55JHZZpttcvbZZy/hEcEXM+bCqzN++J1ZecNu2fbHx6XlGu0zco9jap3RXbFb5xz8x2syZ9rf86veJ9ZsnzDizmw7+PvZ5Yqz8+veJ2Tum29nk+8dkA5bbpwk+eAfHxj5LK8FsLQcf/pNWXft9ul3SM/F9lVV/ffHVqVqydZ92gL4Ar5Q/D777LO54IIL8txzz2WHHXbIsGHDstlmmy227sorr8yiRYs+5hk+0q5du5o/v/baaxkwYEAmTJiQBg0aZIMNNsgJJ5yQ1VdfPUkyZ86cNGvWbLHnWH755TN79uxa26ZNm5a+fftm5syZ2XfffWtdOwzL0rvTZ+bd6TPz9z+9nBkTX0vfR0dm3X2+ledv/eiDHKt/c+MccOdleeuFibl5t6Pz3jv/+m/3/dlzc9s+x2X3a36SE6Y+mEXvL8iLvx2dpy6/OVueemTmvz3zM70WwNJyz73j89tRz2Tcg+d87P6WLZomSWbNfnexfbNmv5vWrT76+7tVi2afsGZ+qqqqap4HloUvFL+PPPJIXnnllVx77bXZfPPNP3Fd+/btl+j5WrRokSlTpmTffffNgAED8te//jU/+9nPsv/+++euu+5KmzZtPtNPg3fffXf22muvvPXWWxk4cGB+/etfL/Es8Fk1adMqXb61ZSY/+ETmvP63mu1vjvtzkqT1WmskSVbZ+Gs56J6r8so9D+f27w6q+ZDav5v8wOP5eafts0L7dnl/1twsmDsvu105OG+/9GoWvfveEr8WwNJ0y2+eyNx572ftzU6t2Vb9jw/wdtnklGyzRdd0XL1NXnn174s99uWJb2b7rddNknRbe5U8NPblj1nzt6zRYcU0adJoGR0BfMFrfrfeeut07do1ffr0ydFHH51HH330Cw1z6aWX5q677spee+2VddZZJzvssEMuv/zyTJ8+PbfffnuSpHnz5pk7d+5ij50zZ85id3rYY489ct5552XIkCFp2rRpjjnmmMyf7zoilo36DRtkrxsvyAaH7Flre7t/3HbsnVf/mqZtW+fA3/0yf/n9w7ltv+M/NnybrdQmG/bplSZtWmXO63/Lgrnz0mC5xum290554bZ7lvi1AJa2H5++d8Y/PDjjHjyn5mvYzw5Pkoy65cQMu/jw7LLjBvnDfRNq7umbJM/96bW89tfp2e1bGyZJdtlpg7w65a08/+LrNWvef39h/nD/hJo1sKx8oTO/G2ywQYYPH56XXnopI0aMSP/+/dO+ffscdNBB2WOPPdKkyUcXrn+Ra35XX331NG3aNFOnTk2SdO7cOVOmTFls3eTJk9OpU6da21Za6aNPm7Zq1SqXXHJJDjjggJx66qm5+OKLXU/EUjf3zbcy7vrfZKvTj8rcaW9l8oNPpOUaq+XbF5+emZOm5sU77s1OQ05J/YYNcu+pQ9JspTa1Hv/BgoV5b+asfLjog3z74jOyTq8dc++pF6WqXr1s95MT8v7suXnsp9ct8WsBLG3tV2212Ifc3p4+J0mydud2WaND25z8/e9kxK/G5vBjr84PB+2Rd2bNz5EnXpvNenTKHt/56L79vXbdJD026JhDjrkyVww5NM1XaJKzL7wjCxZ8kEEDvl3nx0VZlsoH3rp27ZpzzjkngwYNyu23355rrrkmQ4cOzQ9+8IPsvvvuS3TN7/Tp0zN06ND06tUrG2+8cc2+iRMnZv78+TXX/G6zzTb5+c9/npkzZ6ZVq4/+Dzh9+vSMGzcugwYN+sTXWG+99XLWWWfltNNOy2WXXZb+/X0inqXvd0f/KDNfeS3fPKVfvvOLMzN/+juZ8tBTue/0oVk4b346f2vLNGndMt+fOHqxx05+4PFcv+0heXfGOxn+rb7Z4fxB6fvYrUl1dSaNfjTX9Tyk1rXBn/ZaAJXQcfUVc+9vTsnAM2/Ohj3PzHKNG2b3b2+YIef0rvnsTf369fL7WwbmxB/cnJ32GZL3FyzKNzbpnPvvOCWrt2/zKa8AX0xVdfW/3W1/Kamurs5DDz2UuXPnZpdddlnix+29996ZOXNmzjzzzHTp0iWTJ0/O+eefn5kzZ+bOO+9M69atM3fu3Oy2227p1KlTTj755FRXV+e8887La6+9lrvuuqvmw3Bdu3ZNv379Fgvis846KyNHjsyll16aHXbY4VNnmjBhQqZMmZKndxv42f5HAPiS+lH1P+4DPeP6yg4CsJRMeL1HkqR79+6funaZ3P6gqqoq22yzzWcK3yQZNmxYtt5665x99tnZeeedc+aZZ6Z79+751a9+ldatWyf56K4O119/fRo2bJjevXvnwAMPTLNmzXLDDTd87F0g/tPpp5+eDTbYICeddFJefnnxi+0BAPjqWiZnfr9KnPkFvmqc+QW+aip+5hcAAL6MxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxxC8AAMUQvwAAFEP8AgBQDPELAEAxGlR6gP8VF7d6q9IjACwVP/rnH1ofWskxAJae1ycs8VJnfgEK07p160qPAFAxzvwugY4dO2bGKz+t9BgAS0XrLiekdevW3teAr4wpU9qkY8eOS7TWmV8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfqEO7bT3halq0yeTX3urZtvv/jgum+94Tpq075c2XfqnT/+rMn3G3FqPe2PazHz3qCvSpkv/NGnfL1/f6Zw88Mif63p8gCTJ5Nfeyp4HXZwVOhyd1p37Z8+DLv7M72uTX3srBx/9y7Rf7/g0W/3IrLfF6Rl62T358MMP6/pwKIz4hTpyzYiH8sCYF2tt+8N9E7LbgRdng6+tnqfvPSs3Xn5k/nD/n7LbgT9LdXV1kmT+/Pezze7n5elxk/Orawfk8T+emfartMq397soz/3ptUocClCwWbPnp+fu56VF86Z5cvSZ+cNtA/PXN2Zm5/2G5sMPP1yi97V3Zs1Lz93Py6Qpb+VX1w3I+Id+nL4HbZ2BPxyZnwy9u8JHyFfdlyp+L7nkknTt2vVjvyZMmFCzbvbs2TnzzDOzxRZbpHv37unVq1fuv//+Ws918MEHZ7/99lvsNcaPH5+NNtooAwcO9NMldWbam+9k4A9H5ug+29ba/v+G3pWuXVbOL4f2ybrrtM93dtwgVww5NGOffCW/Hz0+STLyN4/nlUl/y7WXHpFtt+qW9ddbPTddeXRatmia/zf0rkocDlCwn1/5f1lh+Sa59tK+WWftVbNpj065+aqjM/j0Xnn//UVL9L52/8MvZvqMeRnxy6PyjU27pPOaK+XEY76d7bdeN7fe8USFj5Cvui9V/CbJyiuvnEceeWSxr27dutWs+f73v59HH300Q4cOzahRo7Llllumf//+eeqpp/7rc0+cODFHHnlkNt9885x//vmpV+9Ld/h8RfU/+cZs9Y2102vXjWttf/q5ydl6i66pqqqq2bbrtzZMgwb188f7//TRmnGTs9xyDfP1TTrXrGncuGG+s8MGNWsA6sqv7nwqvffarNbfoWt1Xjn77L5pmjRptETva3vtunHmvHZF1ujQttZzN2rUIA0a1K+bA6FYDeriRe6556NreHbaaac0aPDfX7J+/fpp27btJ+5/8sknM3bs2FxzzTX5+te/niQ58cQT8/jjj+eyyy7LNddc87GPmzZtWvr27Zu11lorF1988afOAUvLbXc8kfsefiHPj/lJ/jLpzVr7GtSvn/r/8UNY/fr1smKb5fOXSX/7aE2D+qlXr6rWXyRJ0q5t88ya/W7eent22q7YfNkeBECShQsX5YWX3shqq7bOsafcmDt+/2zeX7Ao223VLRed0zurrtJqid7X/tO77y7IiF+Nzb0PvZDrf3FEXRwKBauTU5+NGjXKT37yk2y77bb5xS9+kbfffvtzP9cjjzySxo0bZ/PNN6+1feutt84TTzyRBQsWLPaYmTNnpm/fvmndunUuv/zyNG7c+HO/PnwWM2bOzbGnDs+FZ++f9qu2Wmx/1y4r5/FnJtba9re/z8pbb8/J7Dnv1ayZP39BJrwwtda6P7341ySpWQewrM2YOS+LFn2QH557e1Zss0LuGH5cfnHBwXno0ZeyywE/zYcffrhE72v/rv16x6fpakfmh+fenluvPia9e329rg6HQtVJ/G633Xa57777MmjQoDzwwAPp2bNnBg0alOeee+4zP9err76aVVZZZbEztx06dMjChQszdWrtQJg/f36OOuqoVFdXZ9iwYVl++eW/0LHAZ3H86Tdl3bXbp98hPT92/7H9dsgzz03JT4belffeW5Bpb76TQ465Kss3a5yG//inv+/u+420btUsRw+8PlNfn56FCxfl57/8vzzy2F+SJA0b+idCoG4sXPhBkmSrb6ydH528ZzZav2P22X3TXD7k0Iyb8FpG/d/4JXpf+3cP/+70PHrPD9Kn95bZ/4jLcuMtY+r6sChMnV302qhRo+yxxx657bbbMnz48FRXV+e73/1u9t5774wdO7Zm3XvvvZdzzjknO+20U7bccsv06dMnjz32WM3+OXPmfGzA/nPbrFmzarYtWrQoxx57bJ577rnstNNOad269TI8QqjtnnvH57ejnsmwiw/7xDUH7bdFTj1ul5wz5I4s3+HodN/qB9lrlx5ZpV3LrNR2hSRJi+ZN86trB+SNN99Jh/UHZvkOR2fME3/J9w7bLlVVVVmxtR/ogLrRfIUmSZJNNlyz1vZtvtk1SfL8i68v0fvav+u0xkr5xqZdcu6Z++aQ/b+Z/iffmEWLPlj2B0OxKvKJrw033DAXXXRRRowYkWnTpuW+++5LkjRt2jSNGzdOu3btctFFF+WnP/1pmjZtmsMOO6wmgP/zusf/9O/7n3/++bzzzjs59NBDc+WVVy52RwhYlm75zROZO+/9rL3ZqWmw0uFpsNLh2X6vC5IkXTY5JdvveX6qqqpy7pn7ZubEyzL52SF5888X57ADt8rUN2Zko+4da55r2626ZdIzF2bq+KF5++VLcsvVx+St6bPTtcvKadrUZTxA3WjevEnardR8sXv2fvjhR7cwa9SowRK9rz3z3OSMvP2xxZ5/w691yJy57+XNv81abB8sLRX51NdTTz2VG264IaNHj0737t2zww47JEn69u2bvn371lq70UYbZeedd87VV1+dr3/962nevHlef/31xZ5zzpw5SZIWLVrUbFtjjTVy0003pVGjRpk6dWoGDRqUW2+9NZ07d17s8bC0/fj0vTOw/7drbXvymVdz+PevzqhbTsxandrl8acmZspfp2e/PTfLau0/+peJX9/1VN59d0HNnSH+9vdZGfV/47PbtzesWfPuuwvy67uezoAjtq/bgwKK950dNshvRz2Twaf3qjnh9NCjLyVJundbbYne1/54/59y+o9/nS03X7tmTZJMeOGvady4QVZZuWXdHhRFqbMzvwsWLMjtt9+ePffcM4cffniaNWuW2267LbfccstiH177dw0aNEjnzp1rruXt3Llz3njjjSxcuLDWusmTJ6dRo0ZZbbXVara1aNEijRs3TlVVVS644IKsuOKK+d73vlfr0ghYVtqv2ipf67Zara81O66YJFm7c7us2bFtnnt+ag7od3kuvWp0pkx9O3fd82yOO31E+vfdPmt1XjnJR3d7OO70ETns2GF5/sXXM+GFqdmv7y/SfIXlcsL3vlXJQwQKdOpxu2TK1Onp+/1r8vyLr+cP903IsacOzxabdckOPddbove1PgdsmXYrNc8+h12aRx57ORNf/Xsuu/reXD3ioRzbb4fUr+9WpCw7dXLmd/To0fnhD3+YJk2apHfv3tl3333TqtXin3y/8MIL07Fjx1q/nGLBggV56aWX0qVLlyTJNttsk0svvTRjxoxJz549a9bdf//92XLLLdOoUaOPnWGFFVbIpZdemv322y8nnnhirrzyytSv74NCVNaRh/bMrNnz87Nf/jEDzxyZlVdqkf59t89px+9as6ZN6+Xzh18Nyiln35qv7zQ4VVXJDtuslwfuPDWtWjar4PRAidbusnLu/c3JOemsW7LpDmencaMG+c6O6+enPz4wyZK9r63crmUevPO0nHne7dnv8Msya878rNmhbX7yg31y3FE7VurQKERV9T9/1+AyNHr06CTJtttu+1+D8/zzz8+IESNy2mmnZcstt8w777yTX/7yl7nvvvty7bXX1pwhPuaYY/Liiy/m3HPPzaqrrprhw4fnpptuysiRI7Peeusl+eg3vL3//vu59dZba73GqFGjcsIJJ6RPnz457bTTPnX2f/5mue7tn/lcxw7wZdO6ywlJkhmv/LTCkwAsHXc/2iYdO3ZM9+7dP3VtnZz5/ec1vZ/mpJNOSqtWrXLjjTfmvPPOy/LLL59u3brl5ptvzgYbbFCz7sILL8xFF12U448/PnPnzk23bt1y1VVX1YTvf/Od73wn48ePz7XXXpt11lkne+211+c+LgAA/rfUyZnf/2XO/AJfNc78Al81n+XMryvKAQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAohvgFAKAY4hcAgGKIXwAAiiF+AQAoRlV1dXV1pYf4MnvmmWdSXV2dRo0aVXoUgKViypQplR4BYKlq27ZtGjZsmB49enzq2gZ1MM//tKqqqkqPALBUdezYsdIjACxVCxcuXOJmc+YXAIBiuOYXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF8AAIohfgEAKIb4BQCgGOIXAIBiiF/4knrllVcqPQLAUvXb3/620iOA+IW6dMsttyzRutGjR2f//fdfxtMAfHGDBg3Khx9++F/XVFdX57zzzstpp51WR1PBJxO/UId+9KMfZdiwYf91zeWXX55jjz02a621Vh1NBfD53XfffRkwYEAWLFjwsfvnzJmTfv365brrrkufPn3qdjj4GOIX6tCZZ56ZoUOHZujQoYvte++993L88cfn4osvTu/evXPjjTdWYEKAz+a6667Ls88+m379+mXevHm19k2aNCn77LNPnn766QwdOjSnnHJKhaaEf6mqrq6urvQQUJJRo0bllFNOSa9evXL22WcnSaZNm5ZjjjkmkyZNyllnnZW99tqrwlMCLLmJEyfmiCOOyIorrphhw4alRYsWefDBBzNw4MC0adMmv/jFL9KlS5dKjwlJxC9UxJgxY3Lsscdm2223zT777JMTTzwxTZs2zSWXXJJ111230uMBfGbTpk1L3759U1VVlR133DFXXnllevbsmQsuuCDLL798pceDGuIXKmT8+PE56qij8s477+Sb3/xmhgwZkpYtW1Z6LIDPbebMmTnqqKMyYcKEHHrooTn11FMrPRIsxjW/UCHrr79+RowYkZVXXjlt27YVvsD/vFatWuX666/PFltskaeffjoLFy6s9EiwmAaVHgBK8nEfdNt4441r7n3Ztm3bmu1VVVU54YQT6mo0gM+ld+/ei21buHBhnn/++ey+++5p0aJFrX0jR46sq9HgY7nsAerQOuuss8Rrq6qq8uc//3kZTgPwxR188MGfab072VBp4hcAgGK47AEAWCoWLlyYqVOnZvbs2amqqkqLFi2y2mqrpUEDucGXh/8aoY7Nnj07I0eOzMMPP5xXX301s2bNqvlLonPnztluu+2y7777pkmTJpUeFWCJPP3007nsssvyxBNPZNGiRbX2NWrUKFtttVUGDBjwmS79gmXFZQ9QhyZNmpRDDjkkc+fOzYYbbpgOHTqkWbNmSZK5c+dm8uTJGTduXFZeeeVcf/31WXXVVSs8McB/98ADD6R///7p3r17tt5663To0KHmvr5z5szJ5MmTc//992fixIkZNmxYNt100wpPTOnEL9Shfv36pV69ernwwgvTvHnzj10zffr0DBw4MM2bN8/Pf/7zOp4Q4LPp1atXttpqq0+9O825556b5557zt0eqDj3+YU69PTTT+ekk076xPBNkjZt2uTUU0/No48+WoeTAXw+r7zySvbbb79PXXfQQQe5gw1fCuIX6lBVVVWW5B9b6tWrt0TrACpthRVWyBtvvPGp66ZNm+bXHPOlIH6hDvXo0SNDhgzJ3LlzP3HNrFmzcsEFF2SzzTarw8kAPp/tt98+Z5xxRsaOHZsPP/xwsf0ffPBBHnzwwZx++unZeeedKzAh1OaaX6hDEydOzKGHHpp58+alR48eWW211Wp9MOS1117LuHHj0rp161x//fVZffXVKzwxwH83Z86cHHvssXnsscey3HLLZZVVVqn1vjZt2rQsWLAg2223XYYMGeJONlSc+IU6NnPmzIwYMSKPPvpoXn311cyZMydJ0qJFi3Tq1Ck9e/bM/vvv758Hgf8pTz75ZMaMGVPzvlZVVZXmzZunc+fO2WabbdK9e/dKjwhJxC8AAAXxSy7gS2DGjBn5zW9+k6lTp6ZDhw7p1atXWrZsWemxAD7V888/n27duqVevdofI3riiSdy9dVX17yvHXHEEdlkk00qNCX8izO/UId69OiR0aNHp3Xr1jXbpk6dmt69e2f69Olp2rRp5s+fn5VWWim33XZb2rVrV8FpAT5dt27d8sgjj6RNmzY12x5//PEcdthhWW211dKpU6e8/PLL+fvf/56bb77Z5Q9UnLs9QB2aP3/+Yrcw+9nPfpbWrVtn9OjReeaZZ/K73/0uLVq0yKWXXlqhKQGW3MedQ7viiiuy7bbb5ve//32uuOKK/OEPf0jPnj1zySWXVGBCqE38QoU999xzOeGEE7LaaqslSTp37pxTTjklY8eOrfBkAJ/PX/7ylxx22GGpX79+kqRhw4Y56qij8sILL1R4MhC/UHGLFi2qCd9/WnPNNfPWW29VaCKAL6ZZs2aL3dKsZcuWmTdvXoUmgn8Rv1DHqqqqan3fvXv3xc6GvPLKK7WunwP4sqqqqlrsfW3LLbfMmDFjam0bM2ZM2rdvX5ejwcdytweoYz/+8Y/TuHHjmu9nzJiRa6+9NnvuuWeS5JlnnsngwYPTs2fPygwI8BlUV1dn7733rnW3h/feey+NGzfOkUcemSS57bbbct5552XAgAGVGhNqiF+oQ5tuuulilzPUq1cvHTp0qPn+9ttvT8uWLXPcccfV9XgAn9knBW3Tpk1r/jxlypQceOCBOeKII+pqLPhEbnUGXzIzZsyodSs0AGDpcc0vVNDTTz+dBQsW1PrerzUG/tc9/vjjOffcc/Pkk09WehRYjDO/UEE9evTIHXfckdVXX/1jvwf4X7TPPvtk2rRp6dChQ26++eZKjwO1OPMLFfSfP3v6WRT4Xzd+/Pi89NJLufzyyzN+/Pi8+OKLlR4JahG/AMBSc+ONN+Zb3/pW1l9//Wy//fa54YYbKj0S1CJ+AYClYvr06bnnnntyyCGHJEkOOeSQjBo1KrNmzarwZPAv4hcAWCpuueWWrLvuull//fWTJJtssknWXHPN3HbbbRWeDP5F/AIAX9gHH3yQW265JQcddFCt7QcffHBuvvlmn2ngS0P8AgBf2B//+Md88MEH2XnnnWtt33XXXfPuu+/mvvvuq9BkUJv4hQpq3759GjRo8InfA/yvqFevXgYPHrzYe1ijRo0yePBgZ3750nCfXwAAiuHML1TAnXfemVGjRn3svrvvvvsT9wEAX4z4hQpo2rRpBg8eXOtXGyfJe++9l8GDB/sVxwCwjIhfqIDtttsuTZo0yd13311r+x133JGWLVtm6623rtBkAPDVJn6hAurVq5cDDjggN954Y63tw4cPz4EHHlihqQDgq0/8QoXsu+++mTRpUp566qkkydixY/P6669n7733rvBkAPDVJX6hQlq2bJlddtklw4cPT5LccMMN2W233VzvCwDLkPiFCjrooIMyevToPPnkk3nwwQcX+81IAMDS5T6/UGEHHHBAJk2alLXXXnuxa4ABgKVL/EKFjRs3Lo888ki23nrrrL/++pUeBwC+0sQvAADFcM0vAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMcQvAADFEL8AABRD/AIAUAzxCwBAMf4/vJ/sdHDvwBMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_census)\n",
    "cm.fit(x_census_treinamento,y_census_treinamento)\n",
    "cm.score(x_census_teste,y_census_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.89      0.88      3693\n",
      "        >50K       0.63      0.58      0.61      1192\n",
      "\n",
      "    accuracy                           0.82      4885\n",
      "   macro avg       0.75      0.74      0.74      4885\n",
      "weighted avg       0.81      0.82      0.81      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_teste, previsoes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
